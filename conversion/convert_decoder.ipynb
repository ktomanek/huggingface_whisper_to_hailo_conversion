{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify HuggingFace Transformers Whisper Decoder for Hailo Compatibility\n",
    "\n",
    "Following the Hailo patch modifications:\n",
    "\n",
    "Key decoder modifications:\n",
    "1. Token embedding reshape operations (unsqueeze, transpose, flatten)\n",
    "2. Split final matmul into 4 chunks to avoid Hailo size limits\n",
    "3. Use eager attention (no SDPA)\n",
    "4. Fixed decoder sequence length\n",
    "\n",
    "\n",
    "--> overall: this decoder is not workable quite yet\n",
    "But due to inefficiencies of running a decoder on Hailo NPU, I decided to run the hybrid approach where the decoder is the default one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions\n",
    "from transformers.cache_utils import Cache, DynamicCache, EncoderDecoderCache\n",
    "from transformers.models.whisper.modeling_whisper import create_causal_mask\n",
    "from transformers.utils import logging\n",
    "import onnx\n",
    "import types\n",
    "import os\n",
    "from onnxsim import simplify\n",
    "\n",
    "logger = logging.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"openai/whisper-tiny\"\n",
    "output_dir = \"hailo_compatible_models/hf_whisper_tiny\"\n",
    "\n",
    "# Configuration based on Hailo reference\n",
    "SCALING_FACTOR = 3  # 30s -> 10s\n",
    "INPUT_LENGTH_SECONDS = 10\n",
    "DECODER_SEQUENCE_LENGTH = 32  # Max tokens for tiny model\n",
    "ENCODER_SEQ_LEN = 500  # 1500 / 3 for 10s input\n",
    "HIDDEN_STATES_CHANNELS = 384  # for tiny model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Architecture Modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_matmul_method(self, x):\n",
    "    \"\"\"Split final matmul into 4 chunks to fit Hailo constraints\n",
    "    \n",
    "    From patch lines 108-125:\n",
    "    Splits the large vocab matmul (51865 vocab size) into 4 smaller operations\n",
    "    \"\"\"\n",
    "    vocab_size = self.embed_tokens.weight.shape[0]\n",
    "    chunk_size = vocab_size // 4\n",
    "    logit_chunks = []\n",
    "    \n",
    "    W = self.embed_tokens.weight.to(x.dtype)\n",
    "    \n",
    "    for i in range(4):\n",
    "        start = i * chunk_size\n",
    "        end = (i + 1) * chunk_size if i < 3 else vocab_size  # handle remainder\n",
    "        W_chunk = W[start:end]  # shape: (chunk_size, hidden_size)\n",
    "        logits_chunk = torch.matmul(x, W_chunk.T)  # shape: (batch, seq_len, chunk_size)\n",
    "        logit_chunks.append(logits_chunk)\n",
    "    \n",
    "    logits = torch.cat(logit_chunks, dim=-1)  # shape: (batch, seq_len, vocab_size)\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_decoder_forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        head_mask=None,\n",
    "        cross_attn_head_mask=None,\n",
    "        past_key_values=None,\n",
    "        inputs_embeds=None,\n",
    "        position_ids=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        cache_position=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Copied from original HF implementation, with minor modifications for Hailo compatibility:\n",
    "        https://github.com/huggingface/transformers/blob/53838edde77cb10f3a360150aa85a457637e9ac3/src/transformers/models/whisper/modeling_whisper.py#L765\n",
    "\n",
    "        ONLY modification: Added 3 lines of reshape operations after dropout for Hailo ONNX compatibility\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # retrieve input_ids and inputs_embeds\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "            input_ids = input_ids.view(-1, input_shape[-1])\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embed_tokens(input_ids)\n",
    "\n",
    "        # NOTE: Cache initialization not needed for ONNX export\n",
    "        # Reason: ONNX export is stateless - we process all 32 tokens at once, not autoregressive generation\n",
    "        # The past_key_values cache is only useful for token-by-token generation where we reuse previous\n",
    "        # key/value computations. For fixed-shape ONNX inference, all tokens are provided upfront.\n",
    "        # Additionally, this code block is from a newer transformers version and causes compatibility issues.\n",
    "        #\n",
    "        # if use_cache and past_key_values is None:\n",
    "        #     if self.config.is_encoder_decoder:\n",
    "        #         past_key_values = EncoderDecoderCache(\n",
    "        #             DynamicCache(config=self.config), DynamicCache(config=self.config)\n",
    "        #         )\n",
    "        #     else:\n",
    "        #         past_key_values = DynamicCache(config=self.config)\n",
    "\n",
    "        past_key_values_length = 0\n",
    "        if cache_position is not None:\n",
    "            past_key_values_length = cache_position[0]\n",
    "        elif past_key_values is not None:\n",
    "            past_key_values_length = past_key_values.get_seq_length()\n",
    "\n",
    "        if cache_position is None:\n",
    "            cache_position = torch.arange(\n",
    "                past_key_values_length, past_key_values_length + input_shape[1], device=inputs_embeds.device\n",
    "            )\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = cache_position.unsqueeze(0).repeat(input_shape[0], 1)\n",
    "\n",
    "        # embed positions\n",
    "        if input_ids is not None:\n",
    "            positions = self.embed_positions(\n",
    "                input_ids, past_key_values_length=past_key_values_length, position_ids=position_ids\n",
    "            )\n",
    "        else:\n",
    "            positions = self.embed_positions(\n",
    "                inputs_embeds, past_key_values_length=past_key_values_length, position_ids=position_ids\n",
    "            )\n",
    "\n",
    "        hidden_states = inputs_embeds + positions.to(inputs_embeds.device)\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "\n",
    "        #### START HAILO PATCH (patch lines 144-146) ####\n",
    "        # These reshape operations modify the ONNX graph structure for Hailo compatibility\n",
    "        # The shape returns to [batch, seq_len, hidden_dim] but with different internal layout\n",
    "        hidden_states = hidden_states.unsqueeze(1)  # [B, seq, hidden] -> [B, 1, seq, hidden]\n",
    "        hidden_states = hidden_states.transpose(1, -1)  # [B, 1, seq, hidden] -> [B, hidden, seq, 1]\n",
    "        hidden_states = hidden_states.flatten(2).permute(0, 2, 1)  # -> [B, seq, hidden]\n",
    "        #### END HAILO PATCH ####\n",
    "\n",
    "        causal_mask = create_causal_mask(\n",
    "            config=self.config,\n",
    "            input_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            cache_position=cache_position,\n",
    "            past_key_values=past_key_values,\n",
    "            position_ids=position_ids,\n",
    "        )\n",
    "\n",
    "        if self.gradient_checkpointing and self.training:\n",
    "            if use_cache:\n",
    "                logger.warning_once(\n",
    "                    \"`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\"\n",
    "                )\n",
    "                use_cache = False\n",
    "        # decoder layers\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attns = () if output_attentions else None\n",
    "        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n",
    "\n",
    "        # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n",
    "        for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n",
    "            if attn_mask is not None:\n",
    "                assert attn_mask.size()[0] == (len(self.layers)), (\n",
    "                    f\"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for\"\n",
    "                    f\" {head_mask.size()[0]}.\"\n",
    "                )\n",
    "        for idx, decoder_layer in enumerate(self.layers):\n",
    "            # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "            if self.training:\n",
    "                dropout_probability = torch.rand([])\n",
    "                if dropout_probability < self.layerdrop:\n",
    "                    continue\n",
    "\n",
    "            # Extract past_key_value for this layer (if cache exists)\n",
    "            past_key_value = past_key_values[idx] if past_key_values is not None else None\n",
    "\n",
    "            layer_outputs = decoder_layer(\n",
    "                hidden_states,\n",
    "                attention_mask=causal_mask,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n",
    "                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n",
    "                past_key_value=past_key_value,  # Singular, not plural\n",
    "                output_attentions=output_attentions,\n",
    "                use_cache=use_cache,\n",
    "                cache_position=cache_position,\n",
    "            )\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attns += (layer_outputs[1],)\n",
    "\n",
    "                if encoder_hidden_states is not None:\n",
    "                    all_cross_attentions += (layer_outputs[2],)\n",
    "\n",
    "        hidden_states = self.layer_norm(hidden_states)\n",
    "        # add hidden states from the last decoder layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        next_cache = past_key_values if use_cache else None\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attns,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "def simple_decoder_forward_for_onnx(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        encoder_hidden_states=None,\n",
    "    ):\n",
    "    r\"\"\"\n",
    "    Simplified decoder forward specifically for ONNX export.\n",
    "    Based on OpenAI Whisper structure (patch lines 126-164) adapted for HuggingFace.\n",
    "    \n",
    "    This avoids complex transformers helper functions (like create_causal_mask) \n",
    "    that don't export to ONNX well.\n",
    "    \n",
    "    Uses the same Hailo patch (reshape operations) as new_decoder_forward.\n",
    "    \"\"\"\n",
    "    # Get embeddings\n",
    "    inputs_embeds = self.embed_tokens(input_ids)  # [1, 32, 384]\n",
    "    \n",
    "    # Get positional embeddings (no offset for ONNX - process all tokens at once)\n",
    "    positions = self.embed_positions(input_ids, past_key_values_length=0)  # [1, 32, 384]\n",
    "    \n",
    "    # Combine embeddings\n",
    "    hidden_states = inputs_embeds + positions\n",
    "    hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "    \n",
    "    #### START HAILO PATCH (patch lines 144-146) ####\n",
    "    # These reshape operations modify the ONNX graph structure for Hailo compatibility\n",
    "    hidden_states = hidden_states.unsqueeze(1)  # [B, seq, hidden] -> [B, 1, seq, hidden]\n",
    "    hidden_states = hidden_states.transpose(1, -1)  # [B, 1, seq, hidden] -> [B, hidden, seq, 1]\n",
    "    hidden_states = hidden_states.flatten(2).permute(0, 2, 1)  # -> [B, seq, hidden]\n",
    "    #### END HAILO PATCH ####\n",
    "    \n",
    "    # Create simple causal mask (like OpenAI Whisper's self.mask)\n",
    "    # This is a static mask that's ONNX-compatible\n",
    "    seq_len = input_ids.shape[1]\n",
    "    causal_mask = torch.full((seq_len, seq_len), float(\"-inf\"), device=hidden_states.device, dtype=hidden_states.dtype)\n",
    "    causal_mask = torch.triu(causal_mask, diagonal=1)  # Upper triangular with -inf\n",
    "    causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)  # [1, 1, seq, seq] for batch and heads\n",
    "    \n",
    "    # Process through decoder layers\n",
    "    for decoder_layer in self.layers:\n",
    "        layer_outputs = decoder_layer(\n",
    "            hidden_states,\n",
    "            attention_mask=causal_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            past_key_value=None,  # No cache for ONNX\n",
    "            output_attentions=False,\n",
    "            use_cache=False,\n",
    "        )\n",
    "        hidden_states = layer_outputs[0]\n",
    "    \n",
    "    # Final layer norm\n",
    "    hidden_states = self.layer_norm(hidden_states)  # [1, 32, 384]\n",
    "    \n",
    "    # Return in same format as new_decoder_forward for compatibility\n",
    "    return BaseModelOutputWithPastAndCrossAttentions(\n",
    "        last_hidden_state=hidden_states,\n",
    "        past_key_values=None,\n",
    "        hidden_states=None,\n",
    "        attentions=None,\n",
    "        cross_attentions=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhisperDecoderWrapper(nn.Module):\n",
    "    \"\"\"Wrapper for ONNX export that combines decoder + split matmul\n",
    "    \n",
    "    This is what gets exported to ONNX for Hailo.\n",
    "    Uses our modified decoder (with Hailo patch) and applies split matmul for final logits.\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.decoder = model.model.decoder  # Already has new_decoder_forward with Hailo patch\n",
    "        self.embed_tokens = model.model.decoder.embed_tokens\n",
    "        \n",
    "    def forward(self, decoder_input_ids, encoder_hidden_states):\n",
    "        # Decoder forward (uses our new_decoder_forward with reshape operations)\n",
    "        # Pass minimal arguments to avoid ONNX export issues\n",
    "        decoder_outputs = self.decoder(\n",
    "            input_ids=decoder_input_ids,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            use_cache=False,  # Critical: disable cache for ONNX\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        hidden_states = decoder_outputs.last_hidden_state  # [1, 32, 384]\n",
    "        \n",
    "        # Split matmul for logits (Hailo requirement)\n",
    "        logits = split_matmul_method(self.decoder, hidden_states)  # [1, 32, 51865]\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Apply Modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention implementation: eager\n",
      "Vocab size: 51865\n",
      "Max target positions: 448\n"
     ]
    }
   ],
   "source": [
    "# Load base model with eager attention (no SDPA)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    base_model_name,\n",
    "    attn_implementation='eager'\n",
    ")\n",
    "print(f\"Attention implementation: {model.config._attn_implementation}\")\n",
    "print(f\"Vocab size: {model.config.vocab_size}\")\n",
    "print(f\"Max target positions: {model.config.max_target_positions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying decoder modifications...\n",
      "✓ Decoder modifications applied (new_decoder_forward with Hailo patch)\n",
      "✓ Simplified ONNX-compatible forward attached (simple_decoder_forward_for_onnx)\n"
     ]
    }
   ],
   "source": [
    "# Apply decoder modifications\n",
    "print(\"Applying decoder modifications...\")\n",
    "model.model.decoder.forward = types.MethodType(new_decoder_forward, model.model.decoder)\n",
    "print(\"✓ Decoder modifications applied (new_decoder_forward with Hailo patch)\")\n",
    "\n",
    "# Also attach the simplified version for ONNX export\n",
    "model.model.decoder.forward_for_onnx = types.MethodType(simple_decoder_forward_for_onnx, model.model.decoder)\n",
    "print(\"✓ Simplified ONNX-compatible forward attached (simple_decoder_forward_for_onnx)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Decoder Inference\n",
    "\n",
    "Test the decoder wrapper to ensure it works correctly before ONNX export.\n",
    "This uses the same wrapper that will be exported to ONNX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder hidden states: torch.Size([1, 500, 384])\n",
      "Decoder input IDs: torch.Size([1, 32])\n"
     ]
    }
   ],
   "source": [
    "# Create dummy inputs matching Hailo specs\n",
    "batch_size = 1\n",
    "\n",
    "# Encoder outputs (from 10s audio)\n",
    "encoder_hidden_states = torch.randn(\n",
    "    batch_size,\n",
    "    ENCODER_SEQ_LEN,\n",
    "    HIDDEN_STATES_CHANNELS,\n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "# Decoder input IDs (start token + zeros)\n",
    "decoder_input_ids = torch.cat([\n",
    "    torch.tensor([[50258]], dtype=torch.int64),  # Start token\n",
    "    torch.zeros((1, DECODER_SEQUENCE_LENGTH - 1), dtype=torch.int64)\n",
    "], dim=1)\n",
    "\n",
    "print(f\"Encoder hidden states: {encoder_hidden_states.shape}\")\n",
    "print(f\"Decoder input IDs: {decoder_input_ids.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing decoder wrapper (this is what gets exported to ONNX)...\n",
      "\n",
      "Output logits shape: torch.Size([1, 32, 51865])\n",
      "Expected shape: [1, 32, 51865]\n",
      "Stats: mean=20.190424, std=5.041745\n"
     ]
    }
   ],
   "source": [
    "# Test inference using decoder wrapper (same as ONNX export)\n",
    "print(\"Testing decoder wrapper (this is what gets exported to ONNX)...\")\n",
    "\n",
    "# Create decoder wrapper\n",
    "decoder_wrapper = WhisperDecoderWrapper(model)\n",
    "decoder_wrapper.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = decoder_wrapper(\n",
    "        decoder_input_ids=decoder_input_ids,\n",
    "        encoder_hidden_states=encoder_hidden_states\n",
    "    )\n",
    "\n",
    "print(f\"\\nOutput logits shape: {logits.shape}\")\n",
    "print(f\"Expected shape: [1, {DECODER_SEQUENCE_LENGTH}, {model.config.vocab_size}]\")\n",
    "print(f\"Stats: mean={logits.mean():.6f}, std={logits.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Full vs Simplified Decoder\n",
    "\n",
    "Before ONNX export, verify that the simplified decoder produces similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing full decoder vs simplified decoder...\n",
      "\n",
      "Full decoder logits shape: torch.Size([1, 32, 51865])\n",
      "Simple decoder logits shape: torch.Size([1, 32, 51865])\n",
      "\n",
      "Difference statistics:\n",
      "  Max diff: 0.000000\n",
      "  Mean diff: 0.000000\n",
      "  Median diff: 0.000000\n",
      "\n",
      "✓ Results are very similar! Safe to use simplified version for ONNX.\n"
     ]
    }
   ],
   "source": [
    "# Compare outputs from both decoder approaches\n",
    "print(\"Comparing full decoder vs simplified decoder...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Full decoder (with all transformers features)\n",
    "    output_full = model.model.decoder(\n",
    "        input_ids=decoder_input_ids,\n",
    "        encoder_hidden_states=encoder_hidden_states\n",
    "    )\n",
    "    hidden_full = output_full.last_hidden_state\n",
    "    \n",
    "    # Simplified decoder (ONNX-compatible)\n",
    "    output_simple = model.model.decoder.forward_for_onnx(\n",
    "        input_ids=decoder_input_ids,\n",
    "        encoder_hidden_states=encoder_hidden_states\n",
    "    )\n",
    "    hidden_simple = output_simple.last_hidden_state\n",
    "    \n",
    "    # Compute logits for both\n",
    "    logits_full = split_matmul_method(model.model.decoder, hidden_full)\n",
    "    logits_simple = split_matmul_method(model.model.decoder, hidden_simple)\n",
    "    \n",
    "    # Compare\n",
    "    diff = (logits_full - logits_simple).abs()\n",
    "    print(f\"\\nFull decoder logits shape: {logits_full.shape}\")\n",
    "    print(f\"Simple decoder logits shape: {logits_simple.shape}\")\n",
    "    print(f\"\\nDifference statistics:\")\n",
    "    print(f\"  Max diff: {diff.max().item():.6f}\")\n",
    "    print(f\"  Mean diff: {diff.mean().item():.6f}\")\n",
    "    print(f\"  Median diff: {diff.median().item():.6f}\")\n",
    "    \n",
    "    if diff.max() < 1e-3:\n",
    "        print(\"\\n✓ Results are very similar! Safe to use simplified version for ONNX.\")\n",
    "    elif diff.max() < 0.1:\n",
    "        print(\"\\n⚠ Results are close but have some differences. Should be acceptable for ONNX export.\")\n",
    "    else:\n",
    "        print(\"\\n✗ Results differ significantly! Need to investigate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to ONNX (Hailo Style)\n",
    "\n",
    "Using the simplified decoder for ONNX compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ONNX-compatible wrapper created\n"
     ]
    }
   ],
   "source": [
    "# Create ONNX-compatible wrapper that uses simplified decoder\n",
    "class WhisperDecoderWrapperForONNX(nn.Module):\n",
    "    \"\"\"ONNX-compatible wrapper using simplified decoder forward\"\"\"\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.decoder = model.model.decoder\n",
    "        self.embed_tokens = model.model.decoder.embed_tokens\n",
    "        \n",
    "    def forward(self, decoder_input_ids, encoder_hidden_states):\n",
    "        # Use simplified forward for ONNX compatibility\n",
    "        decoder_outputs = self.decoder.forward_for_onnx(\n",
    "            input_ids=decoder_input_ids,\n",
    "            encoder_hidden_states=encoder_hidden_states\n",
    "        )\n",
    "        \n",
    "        hidden_states = decoder_outputs.last_hidden_state  # [1, 32, 384]\n",
    "        \n",
    "        # Split matmul for logits\n",
    "        logits = split_matmul_method(self.decoder, hidden_states)  # [1, 32, 51865]\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Create wrapper for ONNX export\n",
    "decoder_wrapper_onnx = WhisperDecoderWrapperForONNX(model)\n",
    "decoder_wrapper_onnx.eval()\n",
    "print(\"✓ ONNX-compatible wrapper created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting decoder to: hailo_compatible_models/hf_whisper_tiny/whisper-tiny-decoder-10s-seq-32_base.onnx\n",
      "This may take 2-5 minutes...\n",
      "Torch IR graph at exception: graph(%input.1 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu),\n",
      "      %1 : Float(1, 500, 384, strides=[192000, 384, 1], requires_grad=0, device=cpu),\n",
      "      %decoder.embed_tokens.weight : Float(51865, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.embed_positions.weight : Float(448, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.0.self_attn.k_proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.0.self_attn.v_proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.0.self_attn.v_proj.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.0.self_attn.q_proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.0.self_attn.q_proj.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.0.self_attn.out_proj.weight : Floa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2b/klnl41xn0157xpmx7qvb01cw0000gn/T/ipykernel_8278/3815669020.py:25: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t(384, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.0.self_attn.out_proj.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.0.self_attn_layer_norm.weight : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.0.self_attn_layer_norm.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.0.encoder_attn.k_proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.0.encoder_attn.v_proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.0.encoder_attn.v_proj.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.0.encoder_attn.q_proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.0.encoder_attn.q_proj.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.0.encoder_attn.out_proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.0.encoder_attn.out_proj.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.0.encoder_attn_layer_norm.weight : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.0.encoder_attn_layer_norm.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.0.fc1.weight : Float(1536, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.0.fc1.bias : Float(1536, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.0.fc2.weight : Float(384, 1536, strides=[1536, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.0.fc2.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.0.final_layer_norm.weight : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.0.final_layer_norm.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.1.self_attn.k_proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.1.self_attn.v_proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.1.self_attn.v_proj.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.1.self_attn.q_proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.1.self_attn.q_proj.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.1.self_attn.out_proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.1.self_attn.out_proj.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.1.self_attn_layer_norm.weight : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.1.self_attn_layer_norm.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.1.encoder_attn.k_proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.1.encoder_attn.v_proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.1.encoder_attn.v_proj.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.1.encoder_attn.q_proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.1.encoder_attn.q_proj.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.1.encoder_attn.out_proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.1.encoder_attn.out_proj.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.1.encoder_attn_layer_norm.weight : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.1.encoder_attn_layer_norm.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.1.fc1.weight : Float(1536, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.1.fc1.bias : Float(1536, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.1.fc2.weight : Float(384, 1536, strides=[1536, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.1.fc2.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.1.final_layer_norm.weight : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.1.final_layer_norm.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.2.self_attn.k_proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.2.self_attn.v_proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.2.self_attn.v_proj.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.2.self_attn.q_proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.2.self_attn.q_proj.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.2.self_attn.out_proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.2.self_attn.out_proj.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.2.self_attn_layer_norm.weight : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.2.self_attn_layer_norm.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.2.encoder_attn.k_proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.2.encoder_attn.v_proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.2.encoder_attn.v_proj.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.2.encoder_attn.q_proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.2.encoder_attn.q_proj.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.2.encoder_attn.out_proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.2.encoder_attn.out_proj.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.2.encoder_attn_layer_norm.weight : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.2.encoder_attn_layer_norm.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.2.fc1.weight : Float(1536, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.2.fc1.bias : Float(1536, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.2.fc2.weight : Float(384, 1536, strides=[1536, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.2.fc2.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.2.final_layer_norm.weight : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.2.final_layer_norm.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.3.self_attn.k_proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.3.self_attn.v_proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.3.self_attn.v_proj.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.3.self_attn.q_proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.3.self_attn.q_proj.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.3.self_attn.out_proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.3.self_attn.out_proj.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.3.self_attn_layer_norm.weight : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.3.self_attn_layer_norm.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.3.encoder_attn.k_proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.3.encoder_attn.v_proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.3.encoder_attn.v_proj.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.3.encoder_attn.q_proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.3.encoder_attn.q_proj.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.3.encoder_attn.out_proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.3.encoder_attn.out_proj.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.3.encoder_attn_layer_norm.weight : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.3.encoder_attn_layer_norm.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.3.fc1.weight : Float(1536, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.3.fc1.bias : Float(1536, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.3.fc2.weight : Float(384, 1536, strides=[1536, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.3.fc2.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.3.final_layer_norm.weight : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layers.3.final_layer_norm.bias : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layer_norm.weight : Float(384, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.layer_norm.bias : Float(384, strides=[1], requires_grad=1, device=cpu)):\n",
      "  %3980 : Long(device=cpu) = prim::Constant[value={50257}](), scope: __main__.WhisperDecoderWrapperForONNX::/torch.nn.modules.sparse.Embedding::embed_tokens\n",
      "  %3981 : Bool(device=cpu) = prim::Constant[value={0}](), scope: __main__.WhisperDecoderWrapperForONNX::/torch.nn.modules.sparse.Embedding::embed_tokens\n",
      "  %2931 : Float(1, 32, 384, strides=[12288, 384, 1], requires_grad=0, device=cpu) = aten::embedding(%decoder.embed_tokens.weight, %input.1, %3980, %3981, %3981), scope: __main__.WhisperDecoderWrapperForONNX::/torch.nn.modules.sparse.Embedding::embed_tokens # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/torch/nn/functional.py:2546:0\n",
      "  %3982 : Long(device=cpu) = prim::Constant[value={1}](), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperPositionalEmbedding::embed_positions\n",
      "  %2936 : Long(device=cpu) = aten::size(%input.1, %3982), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperPositionalEmbedding::embed_positions # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:209:0\n",
      "  %3983 : Long(device=cpu) = prim::Constant[value={0}](), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperPositionalEmbedding::embed_positions\n",
      "  %2945 : Float(32, 384, strides=[384, 1], requires_grad=1, device=cpu) = aten::slice(%decoder.embed_positions.weight, %3983, %3983, %2936, %3982), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperPositionalEmbedding::embed_positions # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:209:0\n",
      "  %hidden_states.1 : Float(1, 32, 384, strides=[12288, 384, 1], requires_grad=0, device=cpu) = aten::add(%2931, %2945, %3982), scope: __main__.WhisperDecoderWrapperForONNX:: # /var/folders/2b/klnl41xn0157xpmx7qvb01cw0000gn/T/ipykernel_8278/2765960097.py:193:0\n",
      "  %3984 : Double(device=cpu) = prim::Constant[value={0}](), scope: __main__.WhisperDecoderWrapperForONNX::\n",
      "  %2950 : Float(1, 32, 384, strides=[12288, 384, 1], requires_grad=0, device=cpu) = aten::dropout(%hidden_states.1, %3984, %3981), scope: __main__.WhisperDecoderWrapperForONNX:: # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/torch/nn/functional.py:1422:0\n",
      "  %2952 : Float(1, 1, 32, 384, strides=[12288, 12288, 384, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2950, %3982), scope: __main__.WhisperDecoderWrapperForONNX:: # /var/folders/2b/klnl41xn0157xpmx7qvb01cw0000gn/T/ipykernel_8278/2765960097.py:198:0\n",
      "  %3985 : Long(device=cpu) = prim::Constant[value={-1}](), scope: __main__.WhisperDecoderWrapperForONNX::\n",
      "  %2955 : Float(1, 384, 32, 1, strides=[12288, 1, 384, 12288], requires_grad=0, device=cpu) = aten::transpose(%2952, %3982, %3985), scope: __main__.WhisperDecoderWrapperForONNX:: # /var/folders/2b/klnl41xn0157xpmx7qvb01cw0000gn/T/ipykernel_8278/2765960097.py:199:0\n",
      "  %3986 : Long(device=cpu) = prim::Constant[value={2}](), scope: __main__.WhisperDecoderWrapperForONNX::\n",
      "  %2958 : Float(1, 384, 32, strides=[384, 1, 384], requires_grad=0, device=cpu) = aten::flatten(%2955, %3986, %3985), scope: __main__.WhisperDecoderWrapperForONNX:: # /var/folders/2b/klnl41xn0157xpmx7qvb01cw0000gn/T/ipykernel_8278/2765960097.py:200:0\n",
      "  %3924 : int[] = prim::Constant[value=[0, 2, 1]]()\n",
      "  %input.3 : Float(1, 32, 384, strides=[384, 384, 1], requires_grad=0, device=cpu) = aten::permute(%2958, %3924), scope: __main__.WhisperDecoderWrapperForONNX:: # /var/folders/2b/klnl41xn0157xpmx7qvb01cw0000gn/T/ipykernel_8278/2765960097.py:200:0\n",
      "  %2972 : int[] = prim::ListConstruct(%2936, %2936), scope: __main__.WhisperDecoderWrapperForONNX::\n",
      "  %3987 : Double(device=cpu) = prim::Constant[value={-inf}](), scope: __main__.WhisperDecoderWrapperForONNX::\n",
      "  %3988 : Long(device=cpu) = prim::Constant[value={6}](), scope: __main__.WhisperDecoderWrapperForONNX::\n",
      "  %2975 : NoneType = prim::Constant(), scope: __main__.WhisperDecoderWrapperForONNX::\n",
      "  %2976 : Device = prim::Constant[value=\"cpu\"](), scope: __main__.WhisperDecoderWrapperForONNX:: # /var/folders/2b/klnl41xn0157xpmx7qvb01cw0000gn/T/ipykernel_8278/2765960097.py:206:0\n",
      "  %2978 : Float(32, 32, strides=[32, 1], requires_grad=0, device=cpu) = aten::full(%2972, %3987, %3988, %2975, %2976, %3981), scope: __main__.WhisperDecoderWrapperForONNX:: # /var/folders/2b/klnl41xn0157xpmx7qvb01cw0000gn/T/ipykernel_8278/2765960097.py:206:0\n",
      "  %2980 : Float(32, 32, strides=[32, 1], requires_grad=0, device=cpu) = aten::triu(%2978, %3982), scope: __main__.WhisperDecoderWrapperForONNX:: # /var/folders/2b/klnl41xn0157xpmx7qvb01cw0000gn/T/ipykernel_8278/2765960097.py:207:0\n",
      "  %2982 : Float(1, 32, 32, strides=[1024, 32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2980, %3983), scope: __main__.WhisperDecoderWrapperForONNX:: # /var/folders/2b/klnl41xn0157xpmx7qvb01cw0000gn/T/ipykernel_8278/2765960097.py:208:0\n",
      "  %attention_mask : Float(1, 1, 32, 32, strides=[1024, 1024, 32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2982, %3983), scope: __main__.WhisperDecoderWrapperForONNX:: # /var/folders/2b/klnl41xn0157xpmx7qvb01cw0000gn/T/ipykernel_8278/2765960097.py:208:0\n",
      "  %3925 : int[] = prim::Constant[value=[384]]()\n",
      "  %3989 : Double(device=cpu) = prim::Constant[value={1e-05}](), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/torch.nn.modules.normalization.LayerNorm::self_attn_layer_norm\n",
      "  %3990 : Bool(device=cpu) = prim::Constant[value={1}](), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/torch.nn.modules.normalization.LayerNorm::self_attn_layer_norm\n",
      "  %2989 : Float(1, 32, 384, strides=[12288, 384, 1], requires_grad=0, device=cpu) = aten::layer_norm(%input.3, %3925, %decoder.layers.0.self_attn_layer_norm.weight, %decoder.layers.0.self_attn_layer_norm.bias, %3989, %3990), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/torch.nn.modules.normalization.LayerNorm::self_attn_layer_norm # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/torch/nn/functional.py:2905:0\n",
      "  %2991 : Long(device=cpu) = aten::size(%2989, %3983), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:307:0\n",
      "  %2998 : Long(device=cpu) = aten::size(%2989, %3982), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:307:0\n",
      "  %3005 : Float(1, 32, 384, strides=[12288, 384, 1], requires_grad=0, device=cpu) = aten::linear(%2989, %decoder.layers.0.self_attn.q_proj.weight, %decoder.layers.0.self_attn.q_proj.bias), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn/torch.nn.modules.linear.Linear::q_proj # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %3991 : Double(device=cpu) = prim::Constant[value={0.125}]()\n",
      "  %3941 : Float(1, 32, 384, strides=[12288, 384, 1], requires_grad=0, device=cpu) = aten::mul(%3005, %3991), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:315:0\n",
      "  %3992 : Long(device=cpu) = prim::Constant[value={64}](), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn\n",
      "  %3010 : int[] = prim::ListConstruct(%2991, %2998, %3985, %3992), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn\n",
      "  %3011 : Float(1, 32, 6, 64, strides=[12288, 384, 64, 1], requires_grad=0, device=cpu) = aten::view(%3941, %3010), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:316:0\n",
      "  %3014 : Float(1, 6, 32, 64, strides=[12288, 64, 384, 1], requires_grad=0, device=cpu) = aten::transpose(%3011, %3982, %3986), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:317:0\n",
      "  %query_states.1 : Float(1, 6, 32, 64, strides=[12288, 2048, 64, 1], requires_grad=0, device=cpu) = aten::contiguous(%3014, %3983), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:317:0\n",
      "  %3018 : Float(1, 32, 384, strides=[12288, 384, 1], requires_grad=0, device=cpu) = aten::linear(%2989, %decoder.layers.0.self_attn.k_proj.weight, %2975), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn/torch.nn.modules.linear.Linear::k_proj # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %3022 : int[] = prim::ListConstruct(%2991, %3985, %3988, %3992), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn\n",
      "  %3023 : Float(1, 32, 6, 64, strides=[12288, 384, 64, 1], requires_grad=0, device=cpu) = aten::view(%3018, %3022), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:335:0\n",
      "  %3024 : Float(1, 32, 384, strides=[12288, 384, 1], requires_grad=0, device=cpu) = aten::linear(%2989, %decoder.layers.0.self_attn.v_proj.weight, %decoder.layers.0.self_attn.v_proj.bias), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn/torch.nn.modules.linear.Linear::v_proj # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %3029 : Float(1, 32, 6, 64, strides=[12288, 384, 64, 1], requires_grad=0, device=cpu) = aten::view(%3024, %3022), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:336:0\n",
      "  %3032 : Float(1, 6, 32, 64, strides=[12288, 64, 384, 1], requires_grad=0, device=cpu) = aten::transpose(%3023, %3982, %3986), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:337:0\n",
      "  %key_states.1 : Float(1, 6, 32, 64, strides=[12288, 2048, 64, 1], requires_grad=0, device=cpu) = aten::contiguous(%3032, %3983), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:337:0\n",
      "  %3037 : Float(1, 6, 32, 64, strides=[12288, 64, 384, 1], requires_grad=0, device=cpu) = aten::transpose(%3029, %3982, %3986), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:338:0\n",
      "  %value_states.1 : Float(1, 6, 32, 64, strides=[12288, 2048, 64, 1], requires_grad=0, device=cpu) = aten::contiguous(%3037, %3983), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:338:0\n",
      "  %3993 : Long(device=cpu) = prim::Constant[value={3}](), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn\n",
      "  %3042 : Float(1, 6, 64, 32, strides=[12288, 2048, 1, 64], requires_grad=0, device=cpu) = aten::transpose(%key_states.1, %3986, %3993), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:228:0\n",
      "  %3043 : Float(1, 6, 32, 32, strides=[6144, 1024, 32, 1], requires_grad=0, device=cpu) = aten::matmul(%query_states.1, %3042), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:228:0\n",
      "  %3994 : Double(device=cpu) = prim::Constant[value={1}]()\n",
      "  %3943 : Float(1, 6, 32, 32, strides=[6144, 1024, 32, 1], requires_grad=0, device=cpu) = aten::mul(%3043, %3994), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:228:0\n",
      "  %3053 : Long(device=cpu) = aten::size(%key_states.1, %3986), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:230:0\n",
      "  %3995 : Long(device=cpu) = prim::Constant[value={9223372036854775807}](), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn\n",
      "  %3063 : Float(1, 1, 32, 32, strides=[1024, 1024, 32, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask, %3983, %3983, %3995, %3982), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:230:0\n",
      "  %3068 : Float(1, 1, 32, 32, strides=[1024, 1024, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3063, %3982, %3983, %3995, %3982), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:230:0\n",
      "  %3073 : Float(1, 1, 32, 32, strides=[1024, 1024, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3068, %3986, %3983, %3995, %3982), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:230:0\n",
      "  %3077 : Float(1, 1, 32, 32, strides=[1024, 1024, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3073, %3993, %3983, %3053, %3982), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:230:0\n",
      "  %attn_weights.1 : Float(1, 6, 32, 32, strides=[6144, 1024, 32, 1], requires_grad=0, device=cpu) = aten::add(%3943, %3077, %3982), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:230:0\n",
      "  %attn_weights.3 : Float(1, 6, 32, 32, strides=[6144, 1024, 32, 1], requires_grad=0, device=cpu) = aten::softmax(%attn_weights.1, %3985, %2975), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/torch/nn/functional.py:2137:0\n",
      "  %3085 : Float(1, 6, 32, 32, strides=[6144, 1024, 32, 1], requires_grad=0, device=cpu) = aten::dropout(%attn_weights.3, %3984, %3981), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/torch/nn/functional.py:1422:0\n",
      "  %3086 : Float(1, 6, 32, 64, strides=[12288, 2048, 64, 1], requires_grad=0, device=cpu) = aten::matmul(%3085, %value_states.1), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:238:0\n",
      "  %3089 : Float(1, 32, 6, 64, strides=[12288, 64, 2048, 1], requires_grad=0, device=cpu) = aten::transpose(%3086, %3982, %3986), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:239:0\n",
      "  %3091 : Float(1, 32, 6, 64, strides=[12288, 384, 64, 1], requires_grad=0, device=cpu) = aten::contiguous(%3089, %3983), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:239:0\n",
      "  %3093 : int[] = prim::ListConstruct(%2991, %2998, %3985), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn\n",
      "  %3094 : Float(1, 32, 384, strides=[12288, 384, 1], requires_grad=0, device=cpu) = aten::reshape(%3091, %3093), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:363:0\n",
      "  %3096 : Float(1, 32, 384, strides=[12288, 384, 1], requires_grad=0, device=cpu) = aten::contiguous(%3094, %3983), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:363:0\n",
      "  %hidden_states.3 : Float(1, 32, 384, strides=[12288, 384, 1], requires_grad=0, device=cpu) = aten::linear(%3096, %decoder.layers.0.self_attn.out_proj.weight, %decoder.layers.0.self_attn.out_proj.bias), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn/torch.nn.modules.linear.Linear::out_proj # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %3100 : Float(1, 32, 384, strides=[12288, 384, 1], requires_grad=0, device=cpu) = aten::dropout(%hidden_states.3, %3984, %3981), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0 # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/torch/nn/functional.py:1422:0\n",
      "  %input.5 : Float(1, 32, 384, strides=[12288, 384, 1], requires_grad=0, device=cpu) = aten::add(%input.3, %3100, %3982), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0 # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:514:0\n",
      "  %3107 : Float(1, 32, 384, strides=[12288, 384, 1], requires_grad=0, device=cpu) = aten::layer_norm(%input.5, %3925, %decoder.layers.0.encoder_attn_layer_norm.weight, %decoder.layers.0.encoder_attn_layer_norm.bias, %3989, %3990), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/torch.nn.modules.normalization.LayerNorm::encoder_attn_layer_norm # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/torch/nn/functional.py:2905:0\n",
      "  %3109 : Long(device=cpu) = aten::size(%3107, %3983), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::encoder_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:307:0\n",
      "  %3116 : Long(device=cpu) = aten::size(%3107, %3982), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::encoder_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:307:0\n",
      "  %3123 : Float(1, 32, 384, strides=[12288, 384, 1], requires_grad=0, device=cpu) = aten::linear(%3107, %decoder.layers.0.encoder_attn.q_proj.weight, %decoder.layers.0.encoder_attn.q_proj.bias), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::encoder_attn/torch.nn.modules.linear.Linear::q_proj # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %3996 : Double(device=cpu) = prim::Constant[value={0.125}]()\n",
      "  %3945 : Float(1, 32, 384, strides=[12288, 384, 1], requires_grad=0, device=cpu) = aten::mul(%3123, %3996), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::encoder_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:315:0\n",
      "  %3128 : int[] = prim::ListConstruct(%3109, %3116, %3985, %3992), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::encoder_attn\n",
      "  %3129 : Float(1, 32, 6, 64, strides=[12288, 384, 64, 1], requires_grad=0, device=cpu) = aten::view(%3945, %3128), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::encoder_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:316:0\n",
      "  %3132 : Float(1, 6, 32, 64, strides=[12288, 64, 384, 1], requires_grad=0, device=cpu) = aten::transpose(%3129, %3982, %3986), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::encoder_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:317:0\n",
      "  %query_states.3 : Float(1, 6, 32, 64, strides=[12288, 2048, 64, 1], requires_grad=0, device=cpu) = aten::contiguous(%3132, %3983), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::encoder_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:317:0\n",
      "  %3136 : Float(1, 500, 384, strides=[192000, 384, 1], requires_grad=0, device=cpu) = aten::linear(%1, %decoder.layers.0.encoder_attn.k_proj.weight, %2975), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::encoder_attn/torch.nn.modules.linear.Linear::k_proj # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %3140 : int[] = prim::ListConstruct(%3109, %3985, %3988, %3992), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::encoder_attn\n",
      "  %3141 : Float(1, 500, 6, 64, strides=[192000, 384, 64, 1], requires_grad=0, device=cpu) = aten::view(%3136, %3140), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::encoder_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:335:0\n",
      "  %3142 : Float(1, 500, 384, strides=[192000, 384, 1], requires_grad=0, device=cpu) = aten::linear(%1, %decoder.layers.0.encoder_attn.v_proj.weight, %decoder.layers.0.encoder_attn.v_proj.bias), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::encoder_attn/torch.nn.modules.linear.Linear::v_proj # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %3147 : Float(1, 500, 6, 64, strides=[192000, 384, 64, 1], requires_grad=0, device=cpu) = aten::view(%3142, %3140), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::encoder_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:336:0\n",
      "  %3150 : Float(1, 6, 500, 64, strides=[192000, 64, 384, 1], requires_grad=0, device=cpu) = aten::transpose(%3141, %3982, %3986), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::encoder_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:337:0\n",
      "  %key_states.3 : Float(1, 6, 500, 64, strides=[192000, 32000, 64, 1], requires_grad=0, device=cpu) = aten::contiguous(%3150, %3983), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::encoder_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:337:0\n",
      "  %3155 : Float(1, 6, 500, 64, strides=[192000, 64, 384, 1], requires_grad=0, device=cpu) = aten::transpose(%3147, %3982, %3986), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::encoder_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:338:0\n",
      "  %value_states.3 : Float(1, 6, 500, 64, strides=[192000, 32000, 64, 1], requires_grad=0, device=cpu) = aten::contiguous(%3155, %3983), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::encoder_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:338:0\n",
      "  %3160 : Float(1, 6, 64, 500, strides=[192000, 32000, 1, 64], requires_grad=0, device=cpu) = aten::transpose(%key_states.3, %3986, %3993), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::encoder_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:228:0\n",
      "  %3161 : Float(1, 6, 32, 500, strides=[96000, 16000, 500, 1], requires_grad=0, device=cpu) = aten::matmul(%query_states.3, %3160), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::encoder_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:228:0\n",
      "  %3997 : Double(device=cpu) = prim::Constant[value={1}]()\n",
      "  %attn_weights.5 : Float(1, 6, 32, 500, strides=[96000, 16000, 500, 1], requires_grad=0, device=cpu) = aten::mul(%3161, %3997), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::encoder_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:228:0\n",
      "  %attn_weights.7 : Float(1, 6, 32, 500, strides=[96000, 16000, 500, 1], requires_grad=0, device=cpu) = aten::softmax(%attn_weights.5, %3985, %2975), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::encoder_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/torch/nn/functional.py:2137:0\n",
      "  %3169 : Float(1, 6, 32, 500, strides=[96000, 16000, 500, 1], requires_grad=0, device=cpu) = aten::dropout(%attn_weights.7, %3984, %3981), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::encoder_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/torch/nn/functional.py:1422:0\n",
      "  %3170 : Float(1, 6, 32, 64, strides=[12288, 2048, 64, 1], requires_grad=0, device=cpu) = aten::matmul(%3169, %value_states.3), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::encoder_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:238:0\n",
      "  %3173 : Float(1, 32, 6, 64, strides=[12288, 64, 2048, 1], requires_grad=0, device=cpu) = aten::transpose(%3170, %3982, %3986), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::encoder_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:239:0\n",
      "  %3175 : Float(1, 32, 6, 64, strides=[12288, 384, 64, 1], requires_grad=0, device=cpu) = aten::contiguous(%3173, %3983), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::encoder_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:239:0\n",
      "  %3177 : int[] = prim::ListConstruct(%3109, %3116, %3985), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::encoder_attn\n",
      "  %3178 : Float(1, 32, 384, strides=[12288, 384, 1], requires_grad=0, device=cpu) = aten::reshape(%3175, %3177), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::encoder_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:363:0\n",
      "  %3180 : Float(1, 32, 384, strides=[12288, 384, 1], requires_grad=0, device=cpu) = aten::contiguous(%3178, %3983), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::encoder_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:363:0\n",
      "  %hidden_states.5 : Float(1, 32, 384, strides=[12288, 384, 1], requires_grad=0, device=cpu) = aten::linear(%3180, %decoder.layers.0.encoder_attn.out_proj.weight, %decoder.layers.0.encoder_attn.out_proj.bias), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.models.whisper.modeling_whisper.WhisperAttention::encoder_attn/torch.nn.modules.linear.Linear::out_proj # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %3184 : Float(1, 32, 384, strides=[12288, 384, 1], requires_grad=0, device=cpu) = aten::dropout(%hidden_states.5, %3984, %3981), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0 # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/torch/nn/functional.py:1422:0\n",
      "  %input.7 : Float(1, 32, 384, strides=[12288, 384, 1], requires_grad=0, device=cpu) = aten::add(%input.5, %3184, %3982), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0 # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:530:0\n",
      "  %3191 : Float(1, 32, 384, strides=[12288, 384, 1], requires_grad=0, device=cpu) = aten::layer_norm(%input.7, %3925, %decoder.layers.0.final_layer_norm.weight, %decoder.layers.0.final_layer_norm.bias, %3989, %3990), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/torch.nn.modules.normalization.LayerNorm::final_layer_norm # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/torch/nn/functional.py:2905:0\n",
      "  %3192 : Float(1, 32, 1536, strides=[49152, 1536, 1], requires_grad=0, device=cpu) = aten::linear(%3191, %decoder.layers.0.fc1.weight, %decoder.layers.0.fc1.bias), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/torch.nn.modules.linear.Linear::fc1 # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %3193 : str = prim::Constant[value=\"none\"](), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.activations.GELUActivation::activation_fn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/activations.py:69:0\n",
      "  %hidden_states.7 : Float(1, 32, 1536, strides=[49152, 1536, 1], requires_grad=0, device=cpu) = aten::gelu(%3192, %3193), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/transformers.activations.GELUActivation::activation_fn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/activations.py:69:0\n",
      "  %3197 : Float(1, 32, 1536, strides=[49152, 1536, 1], requires_grad=0, device=cpu) = aten::dropout(%hidden_states.7, %3984, %3981), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0 # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/torch/nn/functional.py:1422:0\n",
      "  %hidden_states.9 : Float(1, 32, 384, strides=[12288, 384, 1], requires_grad=0, device=cpu) = aten::linear(%3197, %decoder.layers.0.fc2.weight, %decoder.layers.0.fc2.bias), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0/torch.nn.modules.linear.Linear::fc2 # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %3201 : Float(1, 32, 384, strides=[12288, 384, 1], requires_grad=0, device=cpu) = aten::dropout(%hidden_states.9, %3984, %3981), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0 # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/torch/nn/functional.py:1422:0\n",
      "  %input.9 : Float(1, 32, 384, strides=[12288, 384, 1], requires_grad=0, device=cpu) = aten::add(%input.7, %3201, %3982), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.0 # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:542:0\n",
      "  %3208 : Float(1, 32, 384, strides=[12288, 384, 1], requires_grad=0, device=cpu) = aten::layer_norm(%input.9, %3925, %decoder.layers.1.self_attn_layer_norm.weight, %decoder.layers.1.self_attn_layer_norm.bias, %3989, %3990), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.1/torch.nn.modules.normalization.LayerNorm::self_attn_layer_norm # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/torch/nn/functional.py:2905:0\n",
      "  %3210 : Long(device=cpu) = aten::size(%3208, %3983), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.1/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:307:0\n",
      "  %3217 : Long(device=cpu) = aten::size(%3208, %3982), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.1/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:307:0\n",
      "  %3224 : Float(1, 32, 384, strides=[12288, 384, 1], requires_grad=0, device=cpu) = aten::linear(%3208, %decoder.layers.1.self_attn.q_proj.weight, %decoder.layers.1.self_attn.q_proj.bias), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.1/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn/torch.nn.modules.linear.Linear::q_proj # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %3998 : Double(device=cpu) = prim::Constant[value={0.125}]()\n",
      "  %3949 : Float(1, 32, 384, strides=[12288, 384, 1], requires_grad=0, device=cpu) = aten::mul(%3224, %3998), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.1/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:315:0\n",
      "  %3229 : int[] = prim::ListConstruct(%3210, %3217, %3985, %3992), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.1/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn\n",
      "  %3230 : Float(1, 32, 6, 64, strides=[12288, 384, 64, 1], requires_grad=0, device=cpu) = aten::view(%3949, %3229), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.1/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:316:0\n",
      "  %3233 : Float(1, 6, 32, 64, strides=[12288, 64, 384, 1], requires_grad=0, device=cpu) = aten::transpose(%3230, %3982, %3986), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.1/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:317:0\n",
      "  %query_states.5 : Float(1, 6, 32, 64, strides=[12288, 2048, 64, 1], requires_grad=0, device=cpu) = aten::contiguous(%3233, %3983), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.1/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:317:0\n",
      "  %3237 : Float(1, 32, 384, strides=[12288, 384, 1], requires_grad=0, device=cpu) = aten::linear(%3208, %decoder.layers.1.self_attn.k_proj.weight, %2975), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.1/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn/torch.nn.modules.linear.Linear::k_proj # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %3241 : int[] = prim::ListConstruct(%3210, %3985, %3988, %3992), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.1/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn\n",
      "  %3242 : Float(1, 32, 6, 64, strides=[12288, 384, 64, 1], requires_grad=0, device=cpu) = aten::view(%3237, %3241), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.1/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:335:0\n",
      "  %3243 : Float(1, 32, 384, strides=[12288, 384, 1], requires_grad=0, device=cpu) = aten::linear(%3208, %decoder.layers.1.self_attn.v_proj.weight, %decoder.layers.1.self_attn.v_proj.bias), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.1/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn/torch.nn.modules.linear.Linear::v_proj # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %3248 : Float(1, 32, 6, 64, strides=[12288, 384, 64, 1], requires_grad=0, device=cpu) = aten::view(%3243, %3241), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.1/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:336:0\n",
      "  %3251 : Float(1, 6, 32, 64, strides=[12288, 64, 384, 1], requires_grad=0, device=cpu) = aten::transpose(%3242, %3982, %3986), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.1/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:337:0\n",
      "  %key_states.5 : Float(1, 6, 32, 64, strides=[12288, 2048, 64, 1], requires_grad=0, device=cpu) = aten::contiguous(%3251, %3983), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.1/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:337:0\n",
      "  %3256 : Float(1, 6, 32, 64, strides=[12288, 64, 384, 1], requires_grad=0, device=cpu) = aten::transpose(%3248, %3982, %3986), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.1/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:338:0\n",
      "  %value_states.5 : Float(1, 6, 32, 64, strides=[12288, 2048, 64, 1], requires_grad=0, device=cpu) = aten::contiguous(%3256, %3983), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.1/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:338:0\n",
      "  %3261 : Float(1, 6, 64, 32, strides=[12288, 2048, 1, 64], requires_grad=0, device=cpu) = aten::transpose(%key_states.5, %3986, %3993), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.1/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:228:0\n",
      "  %3262 : Float(1, 6, 32, 32, strides=[6144, 1024, 32, 1], requires_grad=0, device=cpu) = aten::matmul(%query_states.5, %3261), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.1/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:228:0\n",
      "  %3999 : Double(device=cpu) = prim::Constant[value={1}]()\n",
      "  %3951 : Float(1, 6, 32, 32, strides=[6144, 1024, 32, 1], requires_grad=0, device=cpu) = aten::mul(%3262, %3999), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.1/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:228:0\n",
      "  %3272 : Long(device=cpu) = aten::size(%key_states.5, %3986), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.1/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:230:0\n",
      "  %3296 : Float(1, 1, 32, 32, strides=[1024, 1024, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3073, %3993, %3983, %3272, %3982), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.1/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:230:0\n",
      "  %attn_weights.9 : Float(1, 6, 32, 32, strides=[6144, 1024, 32, 1], requires_grad=0, device=cpu) = aten::add(%3951, %3296, %3982), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.1/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:230:0\n",
      "  %attn_weights.11 : Float(1, 6, 32, 32, strides=[6144, 1024, 32, 1], requires_grad=0, device=cpu) = aten::softmax(%attn_weights.9, %3985, %2975), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.1/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/torch/nn/functional.py:2137:0\n",
      "  %3304 : Float(1, 6, 32, 32, strides=[6144, 1024, 32, 1], requires_grad=0, device=cpu) = aten::dropout(%attn_weights.11, %3984, %3981), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.1/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/torch/nn/functional.py:1422:0\n",
      "  %3305 : Float(1, 6, 32, 64, strides=[12288, 2048, 64, 1], requires_grad=0, device=cpu) = aten::matmul(%3304, %value_states.5), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.1/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:238:0\n",
      "  %3308 : Float(1, 32, 6, 64, strides=[12288, 64, 2048, 1], requires_grad=0, device=cpu) = aten::transpose(%3305, %3982, %3986), scope: __main__.WhisperDecoderWrapperForONNX::/transformers.models.whisper.modeling_whisper.WhisperDecoderLayer::layers.1/transformers.models.whisper.modeling_whisper.WhisperAttention::self_attn # /Users/katrintomanek/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:23"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnsupportedOperatorError\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/torch/onnx/utils.py:1084\u001b[39m, in \u001b[36m_model_to_graph\u001b[39m\u001b[34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[39m\n\u001b[32m   1083\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1084\u001b[39m     graph = \u001b[43m_optimize_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1085\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[43m        \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1087\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_disable_torch_constant_prop\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_disable_torch_constant_prop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1089\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1090\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1091\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1094\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/torch/onnx/utils.py:659\u001b[39m, in \u001b[36m_optimize_graph\u001b[39m\u001b[34m(graph, operator_export_type, _disable_torch_constant_prop, fixed_batch_size, params_dict, dynamic_axes, input_names, module)\u001b[39m\n\u001b[32m    657\u001b[39m _C._jit_pass_onnx_lint(graph)\n\u001b[32m--> \u001b[39m\u001b[32m659\u001b[39m graph = \u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jit_pass_onnx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    660\u001b[39m _C._jit_pass_onnx_lint(graph)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/torch/onnx/utils.py:1738\u001b[39m, in \u001b[36m_run_symbolic_function\u001b[39m\u001b[34m(graph, block, node, inputs, env, values_in_env, new_nodes, operator_export_type)\u001b[39m\n\u001b[32m   1734\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m graph_context.op(\n\u001b[32m   1735\u001b[39m             op_name, *inputs, **attrs, outputs=node.outputsSize()\n\u001b[32m   1736\u001b[39m         )  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1738\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m errors.UnsupportedOperatorError(\n\u001b[32m   1739\u001b[39m         symbolic_function_name,\n\u001b[32m   1740\u001b[39m         opset_version,\n\u001b[32m   1741\u001b[39m         symbolic_function_group.get_min_supported()\n\u001b[32m   1742\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m symbolic_function_group\n\u001b[32m   1743\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1744\u001b[39m     )\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n",
      "\u001b[31mUnsupportedOperatorError\u001b[39m: Exporting the operator 'aten::triu' to ONNX opset version 13 is not supported. Support for this operator was added in version 14, try exporting with this version",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mThis may take 2-5 minutes...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Export to ONNX\u001b[39;00m\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# torch.onnx.export(\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m     \u001b[38;5;66;03m#     verbose=True,  # Set to True if you want to see detailed progress\u001b[39;00m\n\u001b[32m     24\u001b[39m     \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43monnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoder_wrapper_onnx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoder_path_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m13\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Match Hailo reference\u001b[39;49;00m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoder_input_ids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoder_hidden_states\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogits\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to True if you want to see detailed progress\u001b[39;49;00m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ Base ONNX model exported to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecoder_path_base\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/torch/onnx/__init__.py:424\u001b[39m, in \u001b[36mexport\u001b[39m\u001b[34m(model, args, f, kwargs, export_params, verbose, input_names, output_names, opset_version, dynamic_axes, keep_initializers_as_inputs, dynamo, external_data, dynamic_shapes, custom_translation_table, report, optimize, verify, profile, dump_exported_program, artifacts_dir, fallback, training, operator_export_type, do_constant_folding, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[39m\n\u001b[32m    418\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dynamic_shapes:\n\u001b[32m    419\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    420\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe exporter only supports dynamic shapes \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    421\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mthrough parameter dynamic_axes when dynamo=False.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    422\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m \u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m    428\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/torch/onnx/utils.py:522\u001b[39m, in \u001b[36mexport\u001b[39m\u001b[34m(model, args, f, kwargs, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[39m\n\u001b[32m    519\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    520\u001b[39m     args = args + (kwargs,)\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    537\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    538\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/torch/onnx/utils.py:1457\u001b[39m, in \u001b[36m_export\u001b[39m\u001b[34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[39m\n\u001b[32m   1454\u001b[39m     dynamic_axes = {}\n\u001b[32m   1455\u001b[39m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[32m-> \u001b[39m\u001b[32m1457\u001b[39m graph, params_dict, torch_out = \u001b[43m_model_to_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1458\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1459\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1461\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1462\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1463\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_do_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1465\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1468\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1470\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m custom_opsets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1471\u001b[39m     custom_opsets = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/huggingface_whisper_to_hailo_conversion/venv/lib/python3.11/site-packages/torch/onnx/utils.py:1095\u001b[39m, in \u001b[36m_model_to_graph\u001b[39m\u001b[34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[39m\n\u001b[32m   1084\u001b[39m     graph = _optimize_graph(\n\u001b[32m   1085\u001b[39m         graph,\n\u001b[32m   1086\u001b[39m         operator_export_type,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1092\u001b[39m         module=module,\n\u001b[32m   1093\u001b[39m     )\n\u001b[32m   1094\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1095\u001b[39m     \u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jit_onnx_log\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTorch IR graph at exception: \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1096\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m   1098\u001b[39m is_script = \u001b[38;5;28misinstance\u001b[39m(model, (torch.jit.ScriptFunction, torch.jit.ScriptModule))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Export settings\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "decoder_name = f\"whisper-tiny-decoder-{INPUT_LENGTH_SECONDS}s-seq-{DECODER_SEQUENCE_LENGTH}\"\n",
    "decoder_path_base = f\"{output_dir}/{decoder_name}_base.onnx\"\n",
    "decoder_path_final = f\"{output_dir}/{decoder_name}_final.onnx\"\n",
    "\n",
    "print(f\"Exporting decoder to: {decoder_path_base}\")\n",
    "print(\"This may take 2-5 minutes...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Export to ONNX\n",
    "    # torch.onnx.export(\n",
    "    #     decoder_wrapper_onnx,\n",
    "    #     (decoder_input_ids, encoder_hidden_states),\n",
    "    #     decoder_path_base,\n",
    "    #     opset_version=13,  # Match Hailo reference\n",
    "    #     input_names=[\"decoder_input_ids\", \"encoder_hidden_states\"],\n",
    "    #     output_names=[\"logits\"],\n",
    "    #     dynamic_axes=None,  # Fixed shapes for Hailo\n",
    "    #     do_constant_folding=True,\n",
    "    #     export_params=True,\n",
    "    #     verbose=True,  # Set to True if you want to see detailed progress\n",
    "    # )\n",
    "    torch.onnx.export(\n",
    "        decoder_wrapper_onnx,\n",
    "        (decoder_input_ids, encoder_hidden_states),\n",
    "        decoder_path_base,\n",
    "        opset_version=13,  # Match Hailo reference\n",
    "        input_names=[\"decoder_input_ids\", \"encoder_hidden_states\"],\n",
    "        output_names=[\"logits\"],\n",
    "        verbose=True,  # Set to True if you want to see detailed progress\n",
    "    )\n",
    "\n",
    "print(f\"✓ Base ONNX model exported to: {decoder_path_base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplify the ONNX model\n",
    "\n",
    "input_shapes = {\n",
    "    \"decoder_input_ids\": [1, DECODER_SEQUENCE_LENGTH],\n",
    "    \"encoder_hidden_states\": [1, ENCODER_SEQ_LEN, HIDDEN_STATES_CHANNELS]\n",
    "}\n",
    "\n",
    "model_onnx = onnx.load(decoder_path_base)\n",
    "model_simp, check = simplify(model_onnx, overwrite_input_shapes=input_shapes)\n",
    "onnx.save(model_simp, decoder_path_final)\n",
    "\n",
    "# Check if the simplification was successful\n",
    "if check:\n",
    "    logger.info(\"ONNX model was successfully simplified!\")\n",
    "else:\n",
    "    logger.info(\"ONNX model simplification failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View base model\n",
    "! netron {decoder_path_base}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View simplified model\n",
    "! netron {decoder_path_final}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Decoder modifications applied:\n",
    "1. ✓ Token embedding reshape operations (unsqueeze, transpose, flatten)\n",
    "2. ✓ Split final matmul into 4 chunks for Hailo compatibility\n",
    "3. ✓ Eager attention implementation (no SDPA)\n",
    "4. ✓ Fixed sequence length (32 tokens for tiny model)\n",
    "\n",
    "The decoder is now ready for Hailo NPU deployment!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
