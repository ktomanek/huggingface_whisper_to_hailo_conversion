{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ada58e40",
   "metadata": {},
   "source": [
    "# Modify HuggingFace Transformers Whisper to match expectations for Hailo -- ENCODER ONLY \n",
    "\n",
    "Following the patch fill\n",
    "\n",
    "Done \n",
    "\n",
    "* conv1d --> conv2d + forward pass adapted\n",
    "* SDPA_AVAILABLE  --> attn_implementation='eager'\n",
    "* input length 30sec --> 10sec and positional embeddings reconstruction\n",
    "\n",
    "Not done yet:\n",
    "\n",
    "* \"*1.0\" on attention values: see patch v = v.view(*v.shape[:2], self.n_head, -1).permute(0, 2, 1, 3) * 1.0\n",
    "  (would require complete overwrite of WhiseprAttention: https://github.com/huggingface/transformers/blob/53838edde77cb10f3a360150aa85a457637e9ac3/src/transformers/models/whisper/modeling_whisper.py#L288\n",
    "  and then multiply with \"1.0\" here: https://github.com/huggingface/transformers/blob/53838edde77cb10f3a360150aa85a457637e9ac3/src/transformers/models/whisper/modeling_whisper.py#L340\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "882b8623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "import onnx\n",
    "import types\n",
    "import os\n",
    "from onnxsim import simplify\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "589ad58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference model: from hailo's export script for comparison\n",
    "hailo_reference_onnx = \"hailo_reference_models/tiny/tiny-whisper-encoder-10s.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c77b5eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name=\"openai/whisper-tiny\"\n",
    "\n",
    "output_dir=\"hailo_compatible_models/hf_whisper_tiny\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56a61a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go from 30sec --> 10 sec\n",
    "SCALING_FACTOR = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affa9712",
   "metadata": {},
   "source": [
    "# Whisper Encoder Architecture modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a9d708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "def conv2_forward(\n",
    "        self,\n",
    "        input_features,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"Copy from https://github.com/huggingface/transformers/blob/53838edde77cb10f3a360150aa85a457637e9ac3/src/transformers/models/whisper/modeling_whisper.py#L632C5-L730C10\n",
    "        \n",
    "        Modifications for setting conv2d\"\"\"\n",
    "        print(\">> updated forward fn\")\n",
    "        # for orig seq lengh\n",
    "        # p = self.config.max_source_positions\n",
    "        # for modified length\n",
    "        p = self.config.max_source_positions // SCALING_FACTOR\n",
    "        c1 = self.conv1.stride[0]\n",
    "        # this is orig:\n",
    "        # c2 = self.conv2.stride[0]\n",
    "        c2 = self.conv2.stride[1] # (time dimension in 2D stride)\n",
    "        \n",
    "        print(f\"config.max_source_positions: {p}\")\n",
    "        print(f\"self.conv1.stride[0]: {c1}\")\n",
    "        print(f\"self.conv2.stride[0]: {c2}\")\n",
    "\n",
    "        expected_seq_length = p * c1 * c2\n",
    "        print(f\"--> Expected seqlen: {expected_seq_length}\")\n",
    "        if input_features.shape[-1] != expected_seq_length:\n",
    "            raise ValueError(\n",
    "                f\"Whisper expects the mel input features to be of length {expected_seq_length}, but found {input_features.shape[-1]}. Make sure to pad the input mel features to {expected_seq_length}.\"\n",
    "            )\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        #### START HAILO PATCH PART ######\n",
    "\n",
    "        # orig in HF Transformers\n",
    "        # inputs_embeds = nn.functional.gelu(self.conv1(input_features))\n",
    "        # inputs_embeds = nn.functional.gelu(self.conv2(inputs_embeds))\n",
    "        # inputs_embeds = inputs_embeds.permute(0, 2, 1)\n",
    "        \n",
    "        # make compatible with 3D (default whisper) and 4D (what hailo expects)\n",
    "        if len(input_features.shape) == 3:\n",
    "            # handle 3D inputs\n",
    "            print(\"GETTING 3D input...\")\n",
    "            x = input_features.unsqueeze(2)\n",
    "        else:\n",
    "            # handle 4D inputs - this is what Hailo wants\n",
    "            print(\"GETTING 4D input...\")\n",
    "            x = input_features\n",
    "\n",
    "        inputs_embeds = F.gelu(self.conv1(x))\n",
    "        print(f\"--> After conv1: {inputs_embeds.shape}\")\n",
    "        inputs_embeds = F.gelu(self.conv2(inputs_embeds))\n",
    "        print(f\"--> After conv2: {inputs_embeds.shape}\")\n",
    "        inputs_embeds = inputs_embeds.flatten(2).permute(0, 2, 1)  # Hailo patch\n",
    "        print(f\"--> After flatten+permute: {inputs_embeds.shape}\")            \n",
    "        #### END HAILO PATCH PART ######\n",
    "\n",
    "        all_positions = torch.arange(self.embed_positions.num_embeddings, device=inputs_embeds.device)\n",
    "\n",
    "        hidden_states = inputs_embeds + self.embed_positions(all_positions)\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "\n",
    "        encoder_states = () if output_hidden_states else None\n",
    "        all_attentions = () if output_attentions else None\n",
    "\n",
    "        # check if head_mask has a correct number of layers specified if desired\n",
    "        if head_mask is not None:\n",
    "            assert head_mask.size()[0] == (len(self.layers)), (\n",
    "                f\"The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n",
    "            )\n",
    "\n",
    "        for idx, encoder_layer in enumerate(self.layers):\n",
    "            if output_hidden_states:\n",
    "                encoder_states = encoder_states + (hidden_states,)\n",
    "            # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n",
    "            to_drop = False\n",
    "            if self.training:\n",
    "                dropout_probability = torch.rand([])\n",
    "                if dropout_probability < self.layerdrop:  # skip the layer\n",
    "                    to_drop = True\n",
    "\n",
    "            if to_drop:\n",
    "                layer_outputs = (None, None)\n",
    "            else:\n",
    "                layer_outputs = encoder_layer(\n",
    "                    hidden_states,\n",
    "                    None,\n",
    "                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n",
    "                    output_attentions=output_attentions,\n",
    "                )\n",
    "\n",
    "                hidden_states = layer_outputs[0]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_attentions = all_attentions + (layer_outputs[1],)\n",
    "\n",
    "        hidden_states = self.layer_norm(hidden_states)\n",
    "        if output_hidden_states:\n",
    "            encoder_states = encoder_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n",
    "        return BaseModelOutput(\n",
    "            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n",
    "        )    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fab70b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_conv2d(model):\n",
    "    encoder = model.model.encoder\n",
    "    #### Conv1D --> Conv2D conversion ####\n",
    "    # Weight transformations (lines 10-13)\n",
    "    # Conv1d → Conv2d conversion (lines 51-52, 69-70)    \n",
    "    print(\"1️⃣ Applying weight unsqueezing transformation...\")\n",
    "    conv1_weight = encoder.conv1.weight.data.clone()\n",
    "    conv2_weight = encoder.conv2.weight.data.clone()\n",
    "    conv1_bias = encoder.conv1.bias.data.clone() if encoder.conv1.bias is not None else None\n",
    "    conv2_bias = encoder.conv2.bias.data.clone() if encoder.conv2.bias is not None else None\n",
    "\n",
    "    conv1_weight_transformed = conv1_weight.unsqueeze(2)  # add height so we get: [384, 80, 1, 3]\n",
    "    conv2_weight_transformed = conv2_weight.unsqueeze(2)  # dito\n",
    "\n",
    "    print(f\"   Conv1 weight: {conv1_weight.shape} → {conv1_weight_transformed.shape}\")\n",
    "    print(f\"   Conv2 weight: {conv2_weight.shape} → {conv2_weight_transformed.shape}\")\n",
    "\n",
    "    # line 69-74\n",
    "    new_conv1 = nn.Conv2d(\n",
    "        in_channels=80,  # n_mels\n",
    "        out_channels=384,  # n_state\n",
    "        kernel_size=(1, 3),\n",
    "        padding=(0, 1)\n",
    "    )\n",
    "\n",
    "    new_conv2 = nn.Conv2d(\n",
    "        in_channels=384,  # n_state\n",
    "        out_channels=384,  # n_state\n",
    "        kernel_size=(1, 3),\n",
    "        stride=(1, 2),\n",
    "        padding=(0, 1)\n",
    "    )\n",
    "\n",
    "    new_conv1.weight.data = conv1_weight_transformed\n",
    "    new_conv2.weight.data = conv2_weight_transformed\n",
    "\n",
    "    if conv1_bias is not None:\n",
    "        new_conv1.bias.data = conv1_bias\n",
    "    if conv2_bias is not None:\n",
    "        new_conv2.bias.data = conv2_bias\n",
    "\n",
    "    encoder.conv1 = new_conv1\n",
    "    encoder.conv2 = new_conv2\n",
    "\n",
    "    print(\" >> Conv layers converted\")\n",
    "    #### Conv1D --> Conv2D conversion ####\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6c64180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sinusoidal_positions(n_positions, d_model, max_timescale=10000):\n",
    "    # This is the EXACT implementation from OpenAI whisper/model.py\n",
    "    # in openai whisper: sinusoids (https://github.com/openai/whisper/blob/c0d2f624c09dc18e709e37c2ad90c039a4eb72a2/whisper/model.py#L62)\n",
    "    assert d_model % 2 == 0\n",
    "    log_timescale_increment = np.log(max_timescale) / (d_model // 2 - 1)\n",
    "    inv_timescales = torch.exp(-log_timescale_increment * torch.arange(d_model // 2))\n",
    "    scaled_time = torch.arange(n_positions)[:, np.newaxis] * inv_timescales[np.newaxis, :]\n",
    "    return torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9356bb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_positional_scaling(model):\n",
    "    \"\"\"Apply Hailo's n_audio_ctx // 3 scaling to positional embeddings\"\"\"\n",
    "    encoder = model.model.encoder\n",
    "\n",
    "    print(\"Applying positional embedding scaling...\")\n",
    "\n",
    "    # Hailo scaling factor from patch line 171\n",
    "    original_length = 1500  # 30-second audio (max_source_positions)\n",
    "    target_length = original_length // SCALING_FACTOR  # 500 for 10-second audio, TODO make configurable\n",
    "    assert target_length == 500\n",
    "\n",
    "    print(f\"   Scaling positional embeddings: {original_length} → {target_length}\")\n",
    "\n",
    "    # Get original embeddings\n",
    "    original_embeddings = encoder.embed_positions.weight.data\n",
    "    hidden_size = original_embeddings.shape[1]  # Should be 384\n",
    "    assert hidden_size == 384\n",
    "\n",
    "    # Create new embedding layer with reduced size\n",
    "    new_embed_positions = nn.Embedding(target_length, hidden_size)\n",
    "\n",
    "    # # Simple approach: use first 500 embeddings from the original 1500\n",
    "    # new_embed_positions.weight.data = original_embeddings[:target_length].clone()\n",
    "\n",
    "    # alternatively: regenerate sinusoidal on shortened embeddings\n",
    "    sinusoidal_embeddings = create_sinusoidal_positions(target_length, hidden_size)\n",
    "    new_embed_positions.weight = nn.Parameter(sinusoidal_embeddings)\n",
    "\n",
    "    # Replace the embedding layer\n",
    "    encoder.embed_positions = new_embed_positions\n",
    "\n",
    "    print(f\"   Original embedding shape: {original_embeddings.shape}\")\n",
    "    print(f\"   New embedding shape: {new_embed_positions.weight.shape}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5478bc1e",
   "metadata": {},
   "source": [
    "# Load whisper model and apply architecture changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc729611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention implementation: eager\n"
     ]
    }
   ],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(base_model_name, attn_implementation='eager')\n",
    "print(f\"Attention implementation: {model.config._attn_implementation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3876fc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1️⃣ Applying weight unsqueezing transformation...\n",
      "   Conv1 weight: torch.Size([384, 80, 3]) → torch.Size([384, 80, 1, 3])\n",
      "   Conv2 weight: torch.Size([384, 384, 3]) → torch.Size([384, 384, 1, 3])\n",
      " >> Conv layers converted\n"
     ]
    }
   ],
   "source": [
    "model = set_conv2d(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e0a28db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying positional embedding scaling...\n",
      "   Scaling positional embeddings: 1500 → 500\n",
      "   Original embedding shape: torch.Size([1500, 384])\n",
      "   New embedding shape: torch.Size([500, 384])\n"
     ]
    }
   ],
   "source": [
    "model = apply_positional_scaling(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f41001fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = model.model.encoder\n",
    "encoder.forward = types.MethodType(conv2_forward, encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157ce4a2",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97399082",
   "metadata": {},
   "source": [
    "## Basic Inference Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa244d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 80, 1000])\n",
      ">> updated forward fn\n",
      "config.max_source_positions: 500\n",
      "self.conv1.stride[0]: 1\n",
      "self.conv2.stride[0]: 2\n",
      "--> Expected seqlen: 1000\n",
      "GETTING 3D input...\n",
      "--> After conv1: torch.Size([1, 384, 1, 1000])\n",
      "--> After conv2: torch.Size([1, 384, 1, 500])\n",
      "--> After flatten+permute: torch.Size([1, 500, 384])\n",
      "Input shape: torch.Size([1, 80, 1000])\n",
      "Output shape: torch.Size([1, 500, 384])\n",
      "Expected output shape: [1, 1500, 384]\n",
      "Stats: mean=0.012366, std=1.476562\n"
     ]
    }
   ],
   "source": [
    "# test 3D compatibility\n",
    "\n",
    "# we need to change input length\n",
    "LENGTH = 3000 // SCALING_FACTOR\n",
    "test_input = torch.randn(1, 80, LENGTH)  # [batch, n_mels, time_steps]\n",
    "\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoder_output = model.model.encoder(test_input)\n",
    "\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {encoder_output.last_hidden_state.shape}\")\n",
    "print(f\"Expected output shape: [1, 1500, 384]\")  # 3000 → 1500 after conv2 stride=2\n",
    "print(f\"Stats: mean={encoder_output.last_hidden_state.mean():.6f}, std={encoder_output.last_hidden_state.std():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cde92bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 80, 1, 1000])\n",
      ">> updated forward fn\n",
      "config.max_source_positions: 500\n",
      "self.conv1.stride[0]: 1\n",
      "self.conv2.stride[0]: 2\n",
      "--> Expected seqlen: 1000\n",
      "GETTING 4D input...\n",
      "--> After conv1: torch.Size([1, 384, 1, 1000])\n",
      "--> After conv2: torch.Size([1, 384, 1, 500])\n",
      "--> After flatten+permute: torch.Size([1, 500, 384])\n",
      "Input shape: torch.Size([1, 80, 1, 1000])\n",
      "Output shape: torch.Size([1, 500, 384])\n",
      "Expected output shape: [1, 1500, 384]\n",
      "Stats: mean=0.010678, std=1.483264\n"
     ]
    }
   ],
   "source": [
    "# test 4D compatibility\n",
    "\n",
    "# we need to change input length\n",
    "LENGTH = 3000 // SCALING_FACTOR\n",
    "test_input = torch.randn(1, 80, 1, LENGTH)  # [batch, n_mels, time_steps]\n",
    "\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoder_output = model.model.encoder(test_input)\n",
    "\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {encoder_output.last_hidden_state.shape}\")\n",
    "print(f\"Expected output shape: [1, 1500, 384]\")  # 3000 → 1500 after conv2 stride=2\n",
    "print(f\"Stats: mean={encoder_output.last_hidden_state.mean():.6f}, std={encoder_output.last_hidden_state.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0da307",
   "metadata": {},
   "source": [
    "### Compare to original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d141dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention implementation: eager\n"
     ]
    }
   ],
   "source": [
    "orig_model = WhisperForConditionalGeneration.from_pretrained(base_model_name, attn_implementation='eager')\n",
    "print(f\"Attention implementation: {orig_model.config._attn_implementation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4624de2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 80, 3000])\n",
      "Input shape: torch.Size([1, 80, 3000])\n",
      "Output shape: torch.Size([1, 1500, 384])\n",
      "Expected output shape: [1, 1500, 384]\n",
      "Stats: mean=0.020402, std=1.463553\n"
     ]
    }
   ],
   "source": [
    "# Orig whisper model only accepts 3D inputs\n",
    "test_input = torch.randn(1, 80, 3000)  # [batch, n_mels, time_steps]\n",
    "\n",
    "\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoder_output = orig_model.model.encoder(test_input)\n",
    "\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {encoder_output.last_hidden_state.shape}\")\n",
    "print(f\"Expected output shape: [1, 1500, 384]\")  # 3000 → 1500 after conv2 stride=2\n",
    "print(f\"Stats: mean={encoder_output.last_hidden_state.mean():.6f}, std={encoder_output.last_hidden_state.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51ee179",
   "metadata": {},
   "source": [
    "## Compare ONNX Graphs via Netron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "45dcfceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2b/klnl41xn0157xpmx7qvb01cw0000gn/T/ipykernel_4408/4179791145.py:6: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(orig_model.model.encoder, test_input, orig_model_onnx_encoder_path, opset_version=17)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> updated forward fn\n",
      "config.max_source_positions: 500\n",
      "self.conv1.stride[0]: 1\n",
      "self.conv2.stride[0]: 2\n",
      "--> Expected seqlen: 1000\n",
      "GETTING 4D input...\n",
      "--> After conv1: torch.Size([1, 384, 1, 1000])\n",
      "--> After conv2: torch.Size([1, 384, 1, 500])\n",
      "--> After flatten+permute: torch.Size([1, 500, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2b/klnl41xn0157xpmx7qvb01cw0000gn/T/ipykernel_4408/4179791145.py:10: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(model.model.encoder, test_input, patched_model_onnx_encoder_path, opset_version=17)\n",
      "/var/folders/2b/klnl41xn0157xpmx7qvb01cw0000gn/T/ipykernel_4408/994651583.py:30: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_features.shape[-1] != expected_seq_length:\n"
     ]
    }
   ],
   "source": [
    "orig_model_onnx_encoder_path = \"/tmp/original_encoder.onnx\"\n",
    "patched_model_onnx_encoder_path = \"/tmp/patched_encoder.onnx\"\n",
    "\n",
    "# can only do 3D\n",
    "test_input = torch.randn(1, 80, 3000)\n",
    "torch.onnx.export(orig_model.model.encoder, test_input, orig_model_onnx_encoder_path, opset_version=17)\n",
    "\n",
    "# 4D like hailo\n",
    "test_input = torch.randn(1, 80, 1, 1000)\n",
    "torch.onnx.export(model.model.encoder, test_input, patched_model_onnx_encoder_path, opset_version=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ff66535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install netron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5bb9dc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving '/tmp/original_encoder.onnx' at http://localhost:8081\n",
      "^C\n",
      "Stopping http://localhost:8081\n"
     ]
    }
   ],
   "source": [
    "! netron {orig_model_onnx_encoder_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "492e7498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving '/tmp/patched_encoder.onnx' at http://localhost:8081\n",
      "^C\n",
      "Stopping http://localhost:8081\n"
     ]
    }
   ],
   "source": [
    "! netron {patched_model_onnx_encoder_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "646f6467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'converted_models/whisper_onnx_hailo_converted/tiny/tiny-whisper-encoder-10s.onnx' at http://localhost:8081\n",
      "^C\n",
      "Stopping http://localhost:8081\n"
     ]
    }
   ],
   "source": [
    "# compare to model provided by \n",
    "! netron {hailo_reference_onnx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a886b2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af266ed1",
   "metadata": {},
   "source": [
    "# Export the way Hailo does it\n",
    "\n",
    "as from \n",
    "hailo-whisper/export/export_whisper_model.py\n",
    "\n",
    "including onnx simplify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779d0a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "ONNX_ENCODER_MODEL_FILENAME = \"whisper_tiny_encoder_10s_hailo\"\n",
    "\n",
    "# export as 4D model\n",
    "def export_to_onnx_hailo_style(model, output_dir):\n",
    "\n",
    "    # start export\n",
    "    os.makedirs(output_dir, exist_ok=False)\n",
    "    encoder_path_base = f\"{output_dir}/{ONNX_ENCODER_MODEL_FILENAME}_base.onnx\"\n",
    "    encoder_path_final = f\"{output_dir}/{ONNX_ENCODER_MODEL_FILENAME}_final.onnx\"\n",
    "\n",
    "    # 4D\n",
    "    test_input = torch.randn(1, 80, 1, 1000)  # 10s Hailo format\n",
    "\n",
    "    # ensure inference works\n",
    "    with torch.no_grad():\n",
    "        encoder_output = model.model.encoder(test_input)\n",
    "    print(f\"Cncoder_output: {encoder_output.last_hidden_state.shape}\")\n",
    "    print(f\"Stats: mean={encoder_output.last_hidden_state.mean():.6f}, std={encoder_output.last_hidden_state.std():.6f}\")\n",
    "\n",
    "    # Export using EXACT Hailo reference settings\n",
    "    torch.onnx.export(\n",
    "        model.model.encoder,\n",
    "        test_input,\n",
    "        encoder_path_base,\n",
    "        input_names=['x.1'],           # Match Hailo reference input name\n",
    "        output_names=['output_525'],   # For now valid placeholder name (renaming later to what is expected by Hailo)\n",
    "        opset_version=17               # Keep opset 17 as this was used in Hailo exporter\n",
    "    )\n",
    "\n",
    "    # Apply ONNX simplification\n",
    "    print(\"Applying ONNX simplification...\")\n",
    "    model_onnx = onnx.load(encoder_path_base)\n",
    "    model_simp, simplify_successful = simplify(model_onnx)\n",
    "    if not simplify_successful:\n",
    "        raise RuntimeError(\"ONNX simplification failed\")\n",
    "\n",
    "    # Rename output to match Hailo reference exactly\n",
    "    old_name = model_simp.graph.output[0].name\n",
    "    model_simp.graph.output[0].name = \"525\" # somehow this is expected by Hailo\n",
    "\n",
    "    # Update any internal references to the old output name\n",
    "    for node in model_simp.graph.node:\n",
    "        for i, output in enumerate(node.output):\n",
    "            if output == old_name:\n",
    "                print(f\"   Renaming node output {old_name} → 525\")\n",
    "                node.output[i] = \"525\"\n",
    "\n",
    "    # safe final model\n",
    "    onnx.save(model_simp, encoder_path_final)\n",
    "    print(\"Encoder exported:\")\n",
    "    print(f\" * base onnx: {encoder_path_base}\")\n",
    "    print(f\" * simplified onnx: {encoder_path_final}\")\n",
    "\n",
    "    return encoder_path_base, encoder_path_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "351d3740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> updated forward fn\n",
      "config.max_source_positions: 500\n",
      "self.conv1.stride[0]: 1\n",
      "self.conv2.stride[0]: 2\n",
      "--> Expected seqlen: 1000\n",
      "GETTING 4D input...\n",
      "--> After conv1: torch.Size([1, 384, 1, 1000])\n",
      "--> After conv2: torch.Size([1, 384, 1, 500])\n",
      "--> After flatten+permute: torch.Size([1, 500, 384])\n",
      "Cncoder_output: torch.Size([1, 500, 384])\n",
      "Stats: mean=0.010642, std=1.462520\n",
      ">> updated forward fn\n",
      "config.max_source_positions: 500\n",
      "self.conv1.stride[0]: 1\n",
      "self.conv2.stride[0]: 2\n",
      "--> Expected seqlen: 1000\n",
      "GETTING 4D input...\n",
      "--> After conv1: torch.Size([1, 384, 1, 1000])\n",
      "--> After conv2: torch.Size([1, 384, 1, 500])\n",
      "--> After flatten+permute: torch.Size([1, 500, 384])\n",
      "Applying ONNX simplification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2b/klnl41xn0157xpmx7qvb01cw0000gn/T/ipykernel_4408/826676301.py:21: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n",
      "/var/folders/2b/klnl41xn0157xpmx7qvb01cw0000gn/T/ipykernel_4408/994651583.py:30: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_features.shape[-1] != expected_seq_length:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Renaming node output output_525 → 525\n",
      "Encoder exported:\n",
      " * base onnx: hailo_compatible_models/notebook_export/whisper_tiny_encoder_10s_hailo_base\n",
      " * simplified onnx: hailo_compatible_models/notebook_export/whisper_tiny_encoder_10s_hailo_final\n"
     ]
    }
   ],
   "source": [
    "encoder_path_base, encoder_path_final = export_to_onnx_hailo_style(model, \"hailo_compatible_models/notebook_export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983aa709",
   "metadata": {},
   "source": [
    "## Check with Netron again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d61990ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'hailo_compatible_models/notebook_export/whisper_tiny_encoder_10s_hailo_base' at http://localhost:8081\n",
      "^C\n",
      "Stopping http://localhost:8081\n"
     ]
    }
   ],
   "source": [
    "! netron {encoder_path_base}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "db3437e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'hailo_compatible_models/notebook_export/whisper_tiny_encoder_10s_hailo_final' at http://localhost:8081\n",
      "^C\n",
      "Stopping http://localhost:8081\n"
     ]
    }
   ],
   "source": [
    "! netron {encoder_path_final}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a73e0b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'converted_models/whisper_onnx_hailo_converted/tiny/tiny-whisper-encoder-10s.onnx' at http://localhost:8081\n",
      "^C\n",
      "Stopping http://localhost:8081\n"
     ]
    }
   ],
   "source": [
    "! netron {hailo_reference_onnx}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
