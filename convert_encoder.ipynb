{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ada58e40",
   "metadata": {},
   "source": [
    "# Modify HuggingFace Transformers Whisper to match expectations for Hailo -- ENCODER ONLY \n",
    "\n",
    "Following the patch fill\n",
    "\n",
    "Done \n",
    "\n",
    "* conv1d --> conv2d + forward pass adapted\n",
    "* SDPA_AVAILABLE  --> attn_implementation='eager'\n",
    "* input length 30sec --> 10sec and positional embeddings reconstruction\n",
    "\n",
    "Not done yet:\n",
    "\n",
    "* \"*1.0\" on attention values: see patch v = v.view(*v.shape[:2], self.n_head, -1).permute(0, 2, 1, 3) * 1.0\n",
    "  (would require complete overwrite of WhiseprAttention: https://github.com/huggingface/transformers/blob/53838edde77cb10f3a360150aa85a457637e9ac3/src/transformers/models/whisper/modeling_whisper.py#L288\n",
    "  and then multiply with \"1.0\" here: https://github.com/huggingface/transformers/blob/53838edde77cb10f3a360150aa85a457637e9ac3/src/transformers/models/whisper/modeling_whisper.py#L340\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882b8623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "import onnx\n",
    "import types\n",
    "import os\n",
    "from onnxsim import simplify\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589ad58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference model: from hailo's export script for comparison\n",
    "hailo_reference_onnx = \"hailo_reference_models/tiny/tiny-whisper-encoder-10s.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77b5eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name=\"openai/whisper-tiny\"\n",
    "\n",
    "output_dir=\"hailo_compatible_models/hf_whisper_tiny\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a61a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go from 30sec --> 10 sec\n",
    "SCALING_FACTOR = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affa9712",
   "metadata": {},
   "source": [
    "# Whisper Encoder Architecture modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a9d708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "def conv2_forward(\n",
    "        self,\n",
    "        input_features,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"Copy from https://github.com/huggingface/transformers/blob/53838edde77cb10f3a360150aa85a457637e9ac3/src/transformers/models/whisper/modeling_whisper.py#L632C5-L730C10\n",
    "        \n",
    "        Modifications for setting conv2d\"\"\"\n",
    "        print(\">> updated forward fn\")\n",
    "        # for orig seq lengh\n",
    "        # p = self.config.max_source_positions\n",
    "        # for modified length\n",
    "        p = self.config.max_source_positions // SCALING_FACTOR\n",
    "        c1 = self.conv1.stride[0]\n",
    "        # this is orig:\n",
    "        # c2 = self.conv2.stride[0]\n",
    "        c2 = self.conv2.stride[1] # (time dimension in 2D stride)\n",
    "        \n",
    "        print(f\"config.max_source_positions: {p}\")\n",
    "        print(f\"self.conv1.stride[0]: {c1}\")\n",
    "        print(f\"self.conv2.stride[0]: {c2}\")\n",
    "\n",
    "        expected_seq_length = p * c1 * c2\n",
    "        print(f\"--> Expected seqlen: {expected_seq_length}\")\n",
    "        if input_features.shape[-1] != expected_seq_length:\n",
    "            raise ValueError(\n",
    "                f\"Whisper expects the mel input features to be of length {expected_seq_length}, but found {input_features.shape[-1]}. Make sure to pad the input mel features to {expected_seq_length}.\"\n",
    "            )\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        #### START HAILO PATCH PART ######\n",
    "\n",
    "        # orig in HF Transformers\n",
    "        # inputs_embeds = nn.functional.gelu(self.conv1(input_features))\n",
    "        # inputs_embeds = nn.functional.gelu(self.conv2(inputs_embeds))\n",
    "        # inputs_embeds = inputs_embeds.permute(0, 2, 1)\n",
    "        \n",
    "        # make compatible with 3D (default whisper) and 4D (what hailo expects)\n",
    "        if len(input_features.shape) == 3:\n",
    "            # handle 3D inputs\n",
    "            print(\"GETTING 3D input...\")\n",
    "            x = input_features.unsqueeze(2)\n",
    "        else:\n",
    "            # handle 4D inputs - this is what Hailo wants\n",
    "            print(\"GETTING 4D input...\")\n",
    "            x = input_features\n",
    "\n",
    "        inputs_embeds = F.gelu(self.conv1(x))\n",
    "        print(f\"--> After conv1: {inputs_embeds.shape}\")\n",
    "        inputs_embeds = F.gelu(self.conv2(inputs_embeds))\n",
    "        print(f\"--> After conv2: {inputs_embeds.shape}\")\n",
    "        inputs_embeds = inputs_embeds.flatten(2).permute(0, 2, 1)  # Hailo patch\n",
    "        print(f\"--> After flatten+permute: {inputs_embeds.shape}\")            \n",
    "        #### END HAILO PATCH PART ######\n",
    "\n",
    "        all_positions = torch.arange(self.embed_positions.num_embeddings, device=inputs_embeds.device)\n",
    "\n",
    "        hidden_states = inputs_embeds + self.embed_positions(all_positions)\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "\n",
    "        encoder_states = () if output_hidden_states else None\n",
    "        all_attentions = () if output_attentions else None\n",
    "\n",
    "        # check if head_mask has a correct number of layers specified if desired\n",
    "        if head_mask is not None:\n",
    "            assert head_mask.size()[0] == (len(self.layers)), (\n",
    "                f\"The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n",
    "            )\n",
    "\n",
    "        for idx, encoder_layer in enumerate(self.layers):\n",
    "            if output_hidden_states:\n",
    "                encoder_states = encoder_states + (hidden_states,)\n",
    "            # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n",
    "            to_drop = False\n",
    "            if self.training:\n",
    "                dropout_probability = torch.rand([])\n",
    "                if dropout_probability < self.layerdrop:  # skip the layer\n",
    "                    to_drop = True\n",
    "\n",
    "            if to_drop:\n",
    "                layer_outputs = (None, None)\n",
    "            else:\n",
    "                layer_outputs = encoder_layer(\n",
    "                    hidden_states,\n",
    "                    None,\n",
    "                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n",
    "                    output_attentions=output_attentions,\n",
    "                )\n",
    "\n",
    "                hidden_states = layer_outputs[0]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_attentions = all_attentions + (layer_outputs[1],)\n",
    "\n",
    "        hidden_states = self.layer_norm(hidden_states)\n",
    "        if output_hidden_states:\n",
    "            encoder_states = encoder_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n",
    "        return BaseModelOutput(\n",
    "            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n",
    "        )    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab70b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_conv2d(model):\n",
    "    encoder = model.model.encoder\n",
    "    #### Conv1D --> Conv2D conversion ####\n",
    "    # Weight transformations (lines 10-13)\n",
    "    # Conv1d → Conv2d conversion (lines 51-52, 69-70)    \n",
    "    print(\"1️⃣ Applying weight unsqueezing transformation...\")\n",
    "    conv1_weight = encoder.conv1.weight.data.clone()\n",
    "    conv2_weight = encoder.conv2.weight.data.clone()\n",
    "    conv1_bias = encoder.conv1.bias.data.clone() if encoder.conv1.bias is not None else None\n",
    "    conv2_bias = encoder.conv2.bias.data.clone() if encoder.conv2.bias is not None else None\n",
    "\n",
    "    conv1_weight_transformed = conv1_weight.unsqueeze(2)  # add height so we get: [384, 80, 1, 3]\n",
    "    conv2_weight_transformed = conv2_weight.unsqueeze(2)  # dito\n",
    "\n",
    "    print(f\"   Conv1 weight: {conv1_weight.shape} → {conv1_weight_transformed.shape}\")\n",
    "    print(f\"   Conv2 weight: {conv2_weight.shape} → {conv2_weight_transformed.shape}\")\n",
    "\n",
    "    # line 69-74\n",
    "    new_conv1 = nn.Conv2d(\n",
    "        in_channels=80,  # n_mels\n",
    "        out_channels=384,  # n_state\n",
    "        kernel_size=(1, 3),\n",
    "        padding=(0, 1)\n",
    "    )\n",
    "\n",
    "    new_conv2 = nn.Conv2d(\n",
    "        in_channels=384,  # n_state\n",
    "        out_channels=384,  # n_state\n",
    "        kernel_size=(1, 3),\n",
    "        stride=(1, 2),\n",
    "        padding=(0, 1)\n",
    "    )\n",
    "\n",
    "    new_conv1.weight.data = conv1_weight_transformed\n",
    "    new_conv2.weight.data = conv2_weight_transformed\n",
    "\n",
    "    if conv1_bias is not None:\n",
    "        new_conv1.bias.data = conv1_bias\n",
    "    if conv2_bias is not None:\n",
    "        new_conv2.bias.data = conv2_bias\n",
    "\n",
    "    encoder.conv1 = new_conv1\n",
    "    encoder.conv2 = new_conv2\n",
    "\n",
    "    print(\" >> Conv layers converted\")\n",
    "    #### Conv1D --> Conv2D conversion ####\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c64180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sinusoidal_positions(n_positions, d_model, max_timescale=10000):\n",
    "    # This is the EXACT implementation from OpenAI whisper/model.py\n",
    "    # in openai whisper: sinusoids (https://github.com/openai/whisper/blob/c0d2f624c09dc18e709e37c2ad90c039a4eb72a2/whisper/model.py#L62)\n",
    "    assert d_model % 2 == 0\n",
    "    log_timescale_increment = np.log(max_timescale) / (d_model // 2 - 1)\n",
    "    inv_timescales = torch.exp(-log_timescale_increment * torch.arange(d_model // 2))\n",
    "    scaled_time = torch.arange(n_positions)[:, np.newaxis] * inv_timescales[np.newaxis, :]\n",
    "    return torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9356bb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_positional_scaling(model):\n",
    "    \"\"\"Apply Hailo's n_audio_ctx // 3 scaling to positional embeddings\"\"\"\n",
    "    encoder = model.model.encoder\n",
    "\n",
    "    print(\"Applying positional embedding scaling...\")\n",
    "\n",
    "    # Hailo scaling factor from patch line 171\n",
    "    original_length = 1500  # 30-second audio (max_source_positions)\n",
    "    target_length = original_length // SCALING_FACTOR  # 500 for 10-second audio, TODO make configurable\n",
    "    assert target_length == 500\n",
    "\n",
    "    print(f\"   Scaling positional embeddings: {original_length} → {target_length}\")\n",
    "\n",
    "    # Get original embeddings\n",
    "    original_embeddings = encoder.embed_positions.weight.data\n",
    "    hidden_size = original_embeddings.shape[1]  # Should be 384\n",
    "    assert hidden_size == 384\n",
    "\n",
    "    # Create new embedding layer with reduced size\n",
    "    new_embed_positions = nn.Embedding(target_length, hidden_size)\n",
    "\n",
    "    # # Simple approach: use first 500 embeddings from the original 1500\n",
    "    # new_embed_positions.weight.data = original_embeddings[:target_length].clone()\n",
    "\n",
    "    # alternatively: regenerate sinusoidal on shortened embeddings\n",
    "    sinusoidal_embeddings = create_sinusoidal_positions(target_length, hidden_size)\n",
    "    new_embed_positions.weight = nn.Parameter(sinusoidal_embeddings)\n",
    "\n",
    "    # Replace the embedding layer\n",
    "    encoder.embed_positions = new_embed_positions\n",
    "\n",
    "    print(f\"   Original embedding shape: {original_embeddings.shape}\")\n",
    "    print(f\"   New embedding shape: {new_embed_positions.weight.shape}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5478bc1e",
   "metadata": {},
   "source": [
    "# Load whisper model and apply architecture changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc729611",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(base_model_name, attn_implementation='eager')\n",
    "print(f\"Attention implementation: {model.config._attn_implementation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3876fc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = set_conv2d(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0a28db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = apply_positional_scaling(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41001fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = model.model.encoder\n",
    "encoder.forward = types.MethodType(conv2_forward, encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157ce4a2",
   "metadata": {},
   "source": [
    "# Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa244d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 3D compatibility\n",
    "\n",
    "# we need to change input length\n",
    "LENGTH = 3000 // SCALING_FACTOR\n",
    "test_input = torch.randn(1, 80, LENGTH)  # [batch, n_mels, time_steps]\n",
    "\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoder_output = model.model.encoder(test_input)\n",
    "\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {encoder_output.last_hidden_state.shape}\")\n",
    "print(f\"Expected output shape: [1, 1500, 384]\")  # 3000 → 1500 after conv2 stride=2\n",
    "print(f\"Stats: mean={encoder_output.last_hidden_state.mean():.6f}, std={encoder_output.last_hidden_state.std():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde92bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 4D compatibility\n",
    "\n",
    "# we need to change input length\n",
    "LENGTH = 3000 // SCALING_FACTOR\n",
    "test_input = torch.randn(1, 80, 1, LENGTH)  # [batch, n_mels, time_steps]\n",
    "\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoder_output = model.model.encoder(test_input)\n",
    "\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {encoder_output.last_hidden_state.shape}\")\n",
    "print(f\"Expected output shape: [1, 1500, 384]\")  # 3000 → 1500 after conv2 stride=2\n",
    "print(f\"Stats: mean={encoder_output.last_hidden_state.mean():.6f}, std={encoder_output.last_hidden_state.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0da307",
   "metadata": {},
   "source": [
    "### Compare to original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d141dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_model = WhisperForConditionalGeneration.from_pretrained(base_model_name, attn_implementation='eager')\n",
    "print(f\"Attention implementation: {orig_model.config._attn_implementation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4624de2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orig whisper model only accepts 3D inputs\n",
    "test_input = torch.randn(1, 80, 3000)  # [batch, n_mels, time_steps]\n",
    "\n",
    "\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoder_output = orig_model.model.encoder(test_input)\n",
    "\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {encoder_output.last_hidden_state.shape}\")\n",
    "print(f\"Expected output shape: [1, 1500, 384]\")  # 3000 → 1500 after conv2 stride=2\n",
    "print(f\"Stats: mean={encoder_output.last_hidden_state.mean():.6f}, std={encoder_output.last_hidden_state.std():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a886b2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af266ed1",
   "metadata": {},
   "source": [
    "# Export the way Hailo does it\n",
    "\n",
    "as from \n",
    "hailo-whisper/export/export_whisper_model.py\n",
    "\n",
    "including onnx simplify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779d0a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "ONNX_ENCODER_MODEL_FILENAME = \"whisper_tiny_encoder_10s_hailo\"\n",
    "\n",
    "# export as 4D model\n",
    "def export_to_onnx_hailo_style(model, output_dir):\n",
    "\n",
    "    # start export\n",
    "    os.makedirs(output_dir, exist_ok=False)\n",
    "    encoder_path_base = f\"{output_dir}/{ONNX_ENCODER_MODEL_FILENAME}_base.onnx\"\n",
    "    encoder_path_final = f\"{output_dir}/{ONNX_ENCODER_MODEL_FILENAME}_final.onnx\"\n",
    "\n",
    "    # 4D\n",
    "    test_input = torch.randn(1, 80, 1, 1000)  # 10s Hailo format\n",
    "\n",
    "    # ensure inference works\n",
    "    with torch.no_grad():\n",
    "        encoder_output = model.model.encoder(test_input)\n",
    "    print(f\"Cncoder_output: {encoder_output.last_hidden_state.shape}\")\n",
    "    print(f\"Stats: mean={encoder_output.last_hidden_state.mean():.6f}, std={encoder_output.last_hidden_state.std():.6f}\")\n",
    "\n",
    "    # Export using EXACT Hailo reference settings\n",
    "    torch.onnx.export(\n",
    "        model.model.encoder,\n",
    "        test_input,\n",
    "        encoder_path_base,\n",
    "        input_names=['x.1'],           # Match Hailo reference input name\n",
    "        output_names=['output_525'],   # For now valid placeholder name (renaming later to what is expected by Hailo)\n",
    "        opset_version=17               # Keep opset 17 as this was used in Hailo exporter\n",
    "    )\n",
    "\n",
    "    # Apply ONNX simplification\n",
    "    print(\"Applying ONNX simplification...\")\n",
    "    model_onnx = onnx.load(encoder_path_base)\n",
    "    model_simp, simplify_successful = simplify(model_onnx)\n",
    "    if not simplify_successful:\n",
    "        raise RuntimeError(\"ONNX simplification failed\")\n",
    "\n",
    "    # Rename output to match Hailo reference exactly\n",
    "    old_name = model_simp.graph.output[0].name\n",
    "    model_simp.graph.output[0].name = \"525\" # somehow this is expected by Hailo\n",
    "\n",
    "    # Update any internal references to the old output name\n",
    "    for node in model_simp.graph.node:\n",
    "        for i, output in enumerate(node.output):\n",
    "            if output == old_name:\n",
    "                print(f\"   Renaming node output {old_name} → 525\")\n",
    "                node.output[i] = \"525\"\n",
    "\n",
    "    # safe final model\n",
    "    onnx.save(model_simp, encoder_path_final)\n",
    "    print(\"Encoder exported:\")\n",
    "    print(f\" * base onnx: {encoder_path_base}\")\n",
    "    print(f\" * simplified onnx: {encoder_path_final}\")\n",
    "\n",
    "    return encoder_path_base, encoder_path_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351d3740",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_path_base, encoder_path_final = export_to_onnx_hailo_style(model, \"hailo_compatible_models/hf_whisper_tiny\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983aa709",
   "metadata": {},
   "source": [
    "## Check with Netron again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61990ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "! netron {encoder_path_base}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3437e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! netron {encoder_path_final}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73e0b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! netron {hailo_reference_onnx}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
